{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b8da5f2-2472-4d17-bf08-65b9e5144dc7",
   "metadata": {},
   "source": [
    "# Weather Prediction \n",
    "\n",
    "Authors: Feifan Jiang, Liza Kostina, Judy Wu, Daniel Zou\n",
    "\n",
    "Date: November 2024\n",
    "\n",
    "## Objective: \n",
    "The objective of this project is to accurately predict the minimum, average, and maximum daily temperatures (in Fahrenheit) for each of 20 selected cities in the United States, over five future days, across nine consecutive days. These predictions aim to minimize the mean squared error and are submitted daily at noon, starting November 26, 2024, and concluding on December 4, 2024. A total of 2,700 temperature predictions will be made, considering 3 temperature variables, 20 cities, 5 prediction days, and 9 submission days. The project focuses on implementing robust forecasting techniques to achieve precision across diverse geographic locations, from Anchorage to Miami, and varying climates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4e8825-b959-4859-932a-5b47b254128b",
   "metadata": {},
   "source": [
    "## Project timeline\n",
    "\n",
    "* Monday, Nov 25: Model finalized, code committed to Github, Docker image uploaded to Dockerhub\n",
    "* Tuesday, Nov 26: Begin making daily predictions, due at noon daily\n",
    "* Tuesday, Dec 3: Presentations\n",
    "* Wednesday, Dec 4: Final day making predictions; report due"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6665fee9-5cb5-49f0-afb9-ad8eb649e507",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "### Collection and Preprocessing\n",
    "\n",
    "To utilize the strengths of publicly available datasets, we sourced data from NOAA and an additional dataset, leveraging them as follows:\n",
    "\n",
    "NOAA: Used to obtain historical weather data dating back to 1960.\n",
    "???????: Utilized for acquiring recent data. While not restricted by timespan in theory, it was only used to analyze the final year in the presented models.\n",
    "\n",
    "Acquiring data from NOAA was straightforward after converting airport codes to NOAA database codes, using a script detailed in the appendix. The downloaded data is stored in the data/original folder. For further analysis, this data was converted to .csv format, with the processed files available in the data/processed folder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d0e6ec-3bfb-4c3f-8b96-d56ad1788512",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed06f14f-ede1-496e-9d99-00682a22bd33",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d50613-7719-483d-8156-d1be9f6171f8",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "### Further work \n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b89478-8fe0-41b0-9535-1927f8b32902",
   "metadata": {},
   "source": [
    "#### NOAA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7324680-ee9d-41dc-8d30-18b3b17fb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import logging\n",
    "\n",
    "station_code_dict = {\n",
    "    \"PANC\": \"USW00026451\", # Anchorage \n",
    "    \"KBOI\": \"USW00024131\", # Boise  \n",
    "    \"KORD\": \"USW00094846\", # Chicago\n",
    "    \"KDEN\": \"USW00003017\", # Denver \n",
    "    \"KDTW\": \"USW00094847\", # Detroit\n",
    "    \"PHNL\": \"USW00022521\", # Honolulu \n",
    "    \"KIAH\": \"USW00012960\", # Houston\n",
    "    \"KMIA\": \"USW00012839\", # Miami \n",
    "    \"KMSP\": \"USW00014922\", # Minneapolis \n",
    "    \"KOKC\": \"USW00013967\", # Oklahoma City \n",
    "    \"KBNA\": \"USW00013897\", # Nashville \n",
    "    \"KJFK\": \"USW00094789\", # New York \n",
    "    \"KPHX\": \"USW00023183\", # Phoenix \n",
    "    \"KPWM\": \"USW00014764\", # Portland ME\n",
    "    \"KPDX\": \"USW00024229\", # Portland OR \n",
    "    \"KSLC\": \"USW00024127\", # Salt Lake City\n",
    "    \"KSAN\": \"USW00023188\", # San Diego \n",
    "    \"KSFO\": \"USW00023234\", # San Francisco \n",
    "    \"KSEA\": \"USW00024233\", # Seattle \n",
    "    \"KDCA\": \"USW00013743\", # Washington DC\n",
    "}\n",
    "\n",
    "data_path_url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/all/\"\n",
    "\n",
    "# Directory to save downloaded files\n",
    "original_noaa_cache = \"data/original\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(original_noaa_cache, exist_ok=True)\n",
    "\n",
    "# URL to download data from\n",
    "data_path_url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/all/\"\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Loop through the station codes and download the corresponding files\n",
    "for station_code, file_name in station_code_dict.items():\n",
    "    url = f\"{data_path_url}{file_name}.dly\"\n",
    "    try:\n",
    "        # Download the file and save it\n",
    "        urllib.request.urlretrieve(url, os.path.join(original_noaa_cache, f\"{station_code}.dly\"))\n",
    "        logging.info(f\"Successfully scraped data for: {station_code}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to download data for {station_code}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e42ad45-bea9-487f-a332-430fcb83d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the column names and their respective column positions\n",
    "columns = ['ID', 'YEAR', 'MONTH', 'ELEMENT'] + [f'VALUE{i}' for i in range(1, 32)] + [f'MFLAG{i}' for i in range(1, 32)] + [f'QFLAG{i}' for i in range(1, 32)] + [f'SFLAG{i}' for i in range(1, 32)]\n",
    "\n",
    "# Define the fixed column positions for each variable\n",
    "column_positions = [\n",
    "    (0, 11),  # ID (1-11)\n",
    "    (11, 15),  # YEAR (12-15)\n",
    "    (15, 17),  # MONTH (16-17)\n",
    "    (17, 21)  # ELEMENT (18-21)\n",
    "] + [(i*5+21, i*5+26) for i in range(31)] * 3  # VALUE1 to VALUE31, MFLAG1 to MFLAG31, QFLAG1 to QFLAG31, SFLAG1 to SFLAG31\n",
    "\n",
    "\n",
    "# Function to parse a single line of the file\n",
    "def parse_line(line):\n",
    "    data = {}\n",
    "    \n",
    "    # Extract the values for each column based on their positions\n",
    "    data['ID'] = line[0:11].strip()\n",
    "    data['YEAR'] = int(line[11:15].strip())\n",
    "    data['MONTH'] = int(line[15:17].strip())\n",
    "    data['ELEMENT'] = line[17:21].strip()\n",
    "\n",
    "    # Extract VALUE, MFLAG, QFLAG, SFLAG columns\n",
    "    for i in range(31):\n",
    "        start = 21 + i * 8\n",
    "        value_str = line[start:start + 5].strip()\n",
    "        try:\n",
    "            data[f'VALUE{i + 1}'] = int(value_str) if value_str else None\n",
    "        except ValueError:\n",
    "            data[f'VALUE{i + 1}'] = None\n",
    "        data[f'MFLAG{i + 1}'] = line[start + 5:start + 6].strip()\n",
    "        data[f'QFLAG{i + 1}'] = line[start + 6:start + 7].strip()\n",
    "        data[f'SFLAG{i + 1}'] = line[start + 7:start + 8].strip()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_dly_to_dataframe(input_dir, output_dir, parse_line, file_extension=\"csv\"):\n",
    "    \"\"\"\n",
    "    Converts all .dly files from the input directory to DataFrames and saves them in the output directory.\n",
    "    \n",
    "    Parameters:\n",
    "        input_dir (str): Path to the directory containing .dly files.\n",
    "        output_dir (str): Path to the directory where DataFrames will be saved.\n",
    "        parse_line (function): Function that parses a line in the .dly file and returns a dictionary.\n",
    "        file_extension (str): Format to save the DataFrame (e.g., 'csv' or 'parquet'). Defaults to 'csv'.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create output directory if it doesn't exist\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".dly\"):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            \n",
    "            # Read and parse the file into a list of dictionaries\n",
    "            records = []\n",
    "            with open(file_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    record = parse_line(line)\n",
    "                    records.append(record)\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(records)\n",
    "            \n",
    "            # Save the DataFrame\n",
    "            output_file_path = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}.{file_extension}\")\n",
    "            if file_extension == \"csv\":\n",
    "                df.to_csv(output_file_path, index=False)\n",
    "            elif file_extension == \"parquet\":\n",
    "                df.to_parquet(output_file_path, index=False)\n",
    "            \n",
    "            print(f\"Saved {output_file_path}\")\n",
    "\n",
    "\n",
    "input_dir = 'data/original'\n",
    "output_dir = 'data/processed'\n",
    "\n",
    "convert_dly_to_dataframe(input_dir, output_dir, parse_line)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
