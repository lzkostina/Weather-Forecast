{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b8da5f2-2472-4d17-bf08-65b9e5144dc7",
   "metadata": {},
   "source": [
    "# Weather Prediction \n",
    "\n",
    "Authors: Feifan Jiang, Liza Kostina, Judy Wu, Daniel Zou\n",
    "\n",
    "Date: November 2024\n",
    "\n",
    "## Objective: \n",
    "The objective of this project is to accurately predict the minimum, average, and maximum daily temperatures (in Fahrenheit) for each of 20 selected cities in the United States, over five future days, across nine consecutive days. These predictions aim to minimize the mean squared error and are submitted daily at noon, starting November 26, 2024, and concluding on December 4, 2024. A total of 2,700 temperature predictions will be made, considering 3 temperature variables, 20 cities, 5 prediction days, and 9 submission days. The project focuses on implementing robust forecasting techniques to achieve precision across diverse geographic locations, from Anchorage to Miami, and varying climates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4e8825-b959-4859-932a-5b47b254128b",
   "metadata": {},
   "source": [
    "## Project timeline\n",
    "\n",
    "* Monday, Nov 25: Model finalized, code committed to Github, Docker image uploaded to Dockerhub\n",
    "* Tuesday, Nov 26: Begin making daily predictions, due at noon daily\n",
    "* Tuesday, Dec 3: Presentations\n",
    "* Wednesday, Dec 4: Final day making predictions; report due"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6665fee9-5cb5-49f0-afb9-ad8eb649e507",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "### Data Sources  \n",
    "To utilize the strengths of publicly available datasets, we sourced data from NOAA and an additional OpenWeather dataset, leveraging them as follows:\n",
    "\n",
    "* 1. **NOAA Dataset**  \n",
    "   The NOAA dataset provided us with historical weather data dating back to January 1, 1948. Specifically, we utilized the **Global Historical Climatology Network - Daily (GHCN-D)** dataset, which integrates daily climate observations from approximately 30 sources. This comprehensive and consistent dataset includes measurements which we further used in our models:  \n",
    "   - Minimum, maximum, and average temperatures,  \n",
    "   - Precipitation,  \n",
    "   - Snowfall, and  \n",
    "   - Snow depth.  \n",
    "\n",
    "   The NOAA dataset served as the backbone of our analysis, offering a long-term historical perspective essential for training robust prediction models.\n",
    "   \n",
    "* 2. **OpenWeather Dataset**  \n",
    "    To complement the historical data provided by NOAA, we utilized the OpenWeather History API for recent weather data. This dataset offers hourly historical weather data dating back to January 1, 1979, and includes a wide range of weather parameters such as:  \n",
    "    - Minimum, maximum, and average temperatures,  \n",
    "    - Humidity,  \n",
    "    - Precipitation details (rainfall and snowfall), and  \n",
    "    - Wind speed and direction.  \n",
    "\n",
    "For our analysis, we focused on data from 20 locations, specified by their geographic coordinates (longitude and latitude), covering the period from November 12, 2024, to the day of making predictions. The data was retrieved using Python’s `requests` module, downloaded in weekly chunks to stay within the API limits. The raw data was provided in JSON format, requiring additional processing for integration with our models.  \n",
    "\n",
    "The OpenWeather dataset offered high-resolution, recent weather data, providing a contemporary perspective to complement the long-term historical insights from NOAA.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d20f1b-36eb-4fed-a487-705e90a36a00",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Preprocessing both datasets was a critical step in preparing the historical weather data for analysis and model training. \n",
    "\n",
    "*  **NOAA Dataset**\n",
    "    For NOAA dataset the main steps of the preprocessing included:\n",
    "   \n",
    "1. **Format Conversion**  \n",
    "   - The raw data, provided in `.dly` format, was converted into a tabular format to enable easier analysis and integration with our workflow.  \n",
    "\n",
    "2. **Data Cleaning**  \n",
    "   - **Invalid Dates:** Removed entries with invalid dates to ensure consistency.  \n",
    "   - **Missing Average Temperatures:** Filled missing average temperature values by taking the average of the maximum and minimum temperatures for the same day.  \n",
    "   - **Snowfall and Precipitation:** Replaced missing values for snowfall and precipitation with zeros, assuming no snowfall or rainfall occurred.  \n",
    "   - **Invalid Temperature Values:** Fixed anomalous temperature readings (e.g., values below -1000) by replacing them with the previous day’s valid temperature.  \n",
    "\n",
    "3. **Unit Conversion**  \n",
    "   - Converted all temperature values from Celsius to Fahrenheit to align with standard U.S. weather reporting conventions.  \n",
    "\n",
    "4. **Feature Selection**  \n",
    "   - Retained only the core features, including temperature (minimum, maximum, and average), precipitation and snowfall for subsequent model training.  \n",
    "\n",
    "These preprocessing steps ensured the NOAA dataset was clean, consistent, and ready for effective integration into the predictive models.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ad1a55-6032-49dc-b9d3-ee2318204b7b",
   "metadata": {},
   "source": [
    "*  **OpenWeather Dataset**\n",
    "\n",
    "The OpenWeather dataset also underwent several preprocessing steps to prepare it for integration with the NOAA data and subsequent model training. The main steps included:  \n",
    "\n",
    "1. **Format Conversion**  \n",
    "   - Raw data retrieved in `.JSON` format was converted into `.csv` format to facilitate analysis and storage.  \n",
    "\n",
    "2. **Data Aggregation**  \n",
    "   - **Time Conversion:** The UTC timestamps in the dataset were converted to the local time of each city to align with region-specific weather patterns.  \n",
    "   - **Feature Extraction:** Key features, including temperature (minimum, maximum, and average), precipitation, and snowfall records, were extracted for analysis.  \n",
    "   - **Daily Aggregation:** The hourly data was aggregated into daily data by:  \n",
    "     - Calculating the daily average, minimum, and maximum temperatures.  \n",
    "     - Summing total precipitation and snowfall for each day.  \n",
    "\n",
    "3. **Dataset Integration**  \n",
    "   - The processed OpenWeather data, covering the period from November 12 to November 25, 2024, was merged with the processed NOAA data to create a unified dataset for model training.  \n",
    "\n",
    "These preprocessing steps ensured consistency between the two datasets and prepared the data for effective model development. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53919ebf-f905-440d-aba7-ea15ef65ed05",
   "metadata": {},
   "source": [
    "The code used for data downloading and preprocessing is provided in the Appendix section of this report for reference. Additionally, the original code can be found in the analysis folder, and the raw and processed data are stored in the data folder of our GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aeedbf-c28e-4338-9ac3-879be0a9e37f",
   "metadata": {},
   "source": [
    "## Regression-Based Weather Forecasting  \n",
    "\n",
    "After obtaining and preprocessing the data, we framed weather forecasting as a regression problem. By using past weather data as features and future temperature data as labels, we were able to approach this task systematically within a supervised learning framework.  \n",
    "\n",
    "The primary objective of the project was to minimize the Mean Squared Error between the predicted and actual temperature values. Thus, we continued our analysis with preparing regression dataset in the following way: \n",
    "\n",
    "#### Regression Dataset Structure  \n",
    "\n",
    "1. **Features**:  \n",
    "   - A flattened array of the previous 30 days’ data, including:  \n",
    "     - Minimum temperature (TMIN)  \n",
    "     - Average temperature (TAVG)  \n",
    "     - Maximum temperature (TMAX)  \n",
    "     - Snowfall (SNOW)  \n",
    "     - Precipitation (PRCP)  \n",
    "\n",
    "2. **Labels**:  \n",
    "   - A flattened array of the next 5 days’ data, focusing on:  \n",
    "     - Minimum temperature (TMIN)  \n",
    "     - Average temperature (TAVG)  \n",
    "     - Maximum temperature (TMAX)  \n",
    "\n",
    "#### Data Range  \n",
    "\n",
    "To ensure robust model evaluation and fair predictions, the data range was divided as follows:  \n",
    "- **Model Training**:  \n",
    "  - Models were trained on data excluding the evaluation days, with a buffer to prevent any data leakage.  \n",
    "- **Model Evaluation**:  \n",
    "  - To simulate real-world test conditions, evaluation was conducted on data from November 25 to December 10 for each of the past five years.  \n",
    "  - For each evaluation day, predictions were made for the subsequent 5 days.  \n",
    "  - This approach ensured that the evaluation set closely resembled the actual test scenario in timing and structure.  \n",
    "- **Final Prediction**:  \n",
    "  - After selecting the best-performing model, it was trained on the full dataset from January 1, 2014, to October 31, 2024, to generate the final predictions.  \n",
    "\n",
    "This structured approach to training, evaluation, and prediction ensured a clear separation between training and testing phases while leveraging historical and recent data to create a robust predictive framework.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d0e6ec-3bfb-4c3f-8b96-d56ad1788512",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "## Prediction Models"
=======
    "## Models\n",
    "This section outlines the models we implemented for weather forecasting, ranging from simple rule-based predictors to more advanced machine learning techniques. These models aim to predict the minimum, average, and maximum temperatures for the next five days based on historical weather data."
>>>>>>> 7f9679f0b28beed48d0dc9b742bc0bfc4ffac1c0
   ]
  },
  {
   "cell_type": "markdown",
<<<<<<< HEAD
   "id": "ec3899c9-1c69-41cb-a754-7f9ab81e8790",
   "metadata": {},
   "source": [
    "To address the weather forecasting challenge, we trained a series of predictors using diverse methodologies:  \n",
    "\n",
    "- **Previous Day Predictor**  \n",
    "- **Average Last Week Predictor**  \n",
    "- **Linear Regression**  \n",
    "- **Ridge Regression**  \n",
    "- **LASSO**  \n",
    "- **Random Forest**  \n",
    "\n",
    "Additionally, we developed a **Weighted Average Predictor**, which combines the outputs of the above predictors using optimized weights to improve overall accuracy.  \n",
    "\n",
    "#### Baseline Predictors  \n",
    "\n",
    "To establish a benchmark for comparison, we implemented two simple baseline predictors:  \n",
    "\n",
    "1. **Previous Day Predictor**  \n",
    "   - **Approach**: Uses the last recorded day’s weather data to predict the next 5 days.  \n",
    "   - **Mechanism**: Repeats today’s values (TMIN, TAVG, TMAX) for the subsequent 5 days.  \n",
    "   - **Strengths**:  \n",
    "     - Extremely simple and fast.  \n",
    "     - Requires no training.  \n",
    "   - **Limitations**:  \n",
    "     - Assumes static weather patterns, which may lead to inaccuracies during periods of change.  \n",
    "\n",
    "2. **Average Last Week Predictor**  \n",
    "   - **Approach**: Averages the last 7 days of weather data to generate predictions.  \n",
    "   - **Mechanism**: Calculates the mean TMIN, TAVG, and TMAX over the past week and repeats these averages for the next 5 days.  \n",
    "   - **Strengths**:  \n",
    "     - More robust to daily fluctuations compared to the Previous Day Predictor.  \n",
    "   - **Limitations**:  \n",
    "     - Assumes weekly averages accurately reflect future trends, which might not hold during abrupt weather changes.  \n",
    "\n",
    "While both baseline predictors are straightforward to implement, they come with inherent limitations, particularly in their assumptions about weather patterns. These predictors served as useful references, enabling us to evaluate the performance of more sophisticated models.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff148ba-c534-425f-8129-001ae5cb0742",
   "metadata": {},
   "source": [
    "### Regression-Based Predictors  \n",
    "\n",
    "After implementing the baseline predictors, we advanced to regression-based models to better capture complex weather patterns. For these models, the features consisted of the last 30 days of weather data, specifically TMIN, TAVG, TMAX, SNOW, and PRCP (precipitation), which were flattened into a single feature vector of size 150.\n",
    "\n",
    "#### Linear Regression  \n",
    "- **Strength**:  \n",
    "  - Simple, interpretable, and computationally efficient.  \n",
    "  - Provides clear coefficients that explain the relationship between features and the target variable.  \n",
    "- **Limitation**:  \n",
    "  - Assumes a linear relationship between features and the target, which limits its ability to model more complex weather patterns or interactions between variables.\n",
    "\n",
    "#### Ridge Regression (5-fold with RidgeCV)  \n",
    "- **Approach**:  \n",
    "  - Ridge regression was applied with a regularization parameter $(\\alpha)$ tuned via 5-fold cross-validation using RidgeCV.  \n",
    "- **Strength**:  \n",
    "  - Handles multicollinearity effectively, reducing the potential for overfitting by shrinking the coefficients.  \n",
    "- **Limitation**:  \n",
    "  - Despite regularization, it still assumes linear relationships between the features and target, which may not fully capture nonlinear weather patterns.\n",
    "\n",
    "#### LASSO Regression (5-fold with LassoCV)  \n",
    "- **Approach**:  \n",
    "  - LASSO regression was also tuned using 5-fold cross-validation with LassoCV.  \n",
    "- **Strength**:  \n",
    "  - Particularly useful for feature selection, as it tends to shrink coefficients of less important features to zero.  \n",
    "- **Limitation**:  \n",
    "  - LASSO may drop important features if they are highly correlated with others, which could result in the loss of valuable predictors.\n",
    "\n",
    "These regression-based models introduced more complexity compared to the baseline predictors, and their strengths and limitations reflect the trade-off between interpretability and the ability to capture complex relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da27119e-b991-4652-9cb6-30ec12f2c4d7",
   "metadata": {},
   "source": [
    "### Random Forest and Weighted Average Predictor  \n",
    "\n",
    "After exploring linear and regularized regression models, we turned to more complex machine learning models, such as Random Forest, to capture nonlinear relationships in the data.\n",
    "\n",
    "#### Random Forest (GridSearchCV with 5-fold Cross-Validation)  \n",
    "- **Features**: Same as Linear Regression (150 features, including TMIN, TAVG, TMAX, SNOW, and PRCP for the past 30 days).  \n",
    "- **Strength**:  \n",
    "  - Random Forests excel at handling complex patterns and interactions between features, which are common in weather data.  \n",
    "  - Provides feature importance, allowing us to identify which variables contribute most to the model’s predictions.  \n",
    "- **Limitation**:  \n",
    "  - Computationally intensive, especially as the number of trees and data size increases. Training and prediction times may be longer compared to simpler models.\n",
    "\n",
    "#### Weighted Average Predictor (Custom Cross-Validation over Weights)  \n",
    "- **Approach**:  \n",
    "  - The Weighted Average Predictor combines predictions from multiple models by calculating a weighted average of their outputs.  \n",
    "  - Weights for each model’s prediction are generated randomly using a Dirichlet distribution, and each weight combination is evaluated based on the Mean Squared Error (MSE). The best-performing weights are then selected.  \n",
    "- **Features and Targets**:  \n",
    "  - Features and targets are the same as those used in the individual models, with weights applied to the predictions of each model.  \n",
    "- **Strength**:  \n",
    "  - By combining predictions from multiple models, this approach improves accuracy and captures complementary patterns that individual models may miss.  \n",
    "- **Limitation**:  \n",
    "  - The quality of the model depends heavily on the choice of weights, which could lead to suboptimal performance if not carefully optimized.\n",
    "\n",
    "These models represent a natural progression from simpler linear models to more sophisticated ensemble methods, with the goal of improving prediction accuracy by capturing complex relationships in the data. The Random Forest provides a powerful tool for identifying complex patterns, while the Weighted Average Predictor serves as a way to combine multiple model outputs for enhanced performance.\n"
=======
   "id": "434b7371",
   "metadata": {},
   "source": [
    "### Previous Day Predictor\n",
    "We applied the Previous Day Predictor, a simple rule-based model that uses the most recent weather observations as predictions for future days. Specifically, it extracts the minimum temperature (TMIN), average temperature (TAVG), and maximum temperature (TMAX) from the last recorded day and extrapolates these values for the next 5 days. This approach is computationally efficient and requires no training, making it suitable as a baseline model. However, it assumes that weather patterns remain constant, limiting its accuracy in dynamic conditions.\n",
    "\n",
    "\n",
    "### Average Last Week Predictor\n",
    "We implemented the Average Last Week Predictor to generate weather forecasts based on historical trends. This method calculates the average values of TMIN, TAVG, and TMAX over the previous 7 days and uses these averages as predictions for the next 5 days. The averaged values are repeated to produce 15 predictions.\n",
    "\n",
    "By aggregating data from the last week, this predictor provides greater stability and reduces the influence of single-day anomalies. However, it assumes that the past week's average sufficiently represents future weather, which may lead to inaccuracies during periods of rapid weather fluctuation.\n",
    "\n",
    "### Linear Regression\n",
    "We fit a Linear Regression Model to the weather dataset, using the last 30 days of historical data as input features and future temperature values as targets. The input features include daily measurements of TMIN, TAVG, TMAX, SNOW, and PRCP, flattened into a 1D array. The model learns a linear relationship between these features and the predicted temperatures over the next 5 days.\n",
    "\n",
    "This method is simple and interpretable, providing insight into the relationship between input features and predictions. However, its reliance on linear assumptions makes it less effective in capturing complex, non-linear patterns in weather data.\n",
    "\n",
    "### Ridge Regression\n",
    "We applied Ridge Regression to enhance the performance of linear regression by introducing L2-regularization, which penalizes large coefficients and improves model generalization. Using RidgeCV, we optimized the regularization strength ($\\alpha$) through 5-fold cross-validation, selecting the value that minimized the mean squared error (MSE).\n",
    "\n",
    "The Ridge Regression model effectively handles multicollinearity among features and performs well for datasets with interdependent variables. However, it still assumes a linear relationship, which limits its ability to capture non-linear trends in weather patterns.\n",
    "\n",
    "### Lasso Regression\n",
    "We used Lasso Regression, which incorporates L1-regularization to perform feature selection by shrinking some coefficients to zero. The model uses weather features from the previous 30 days as input and predicts temperature values for the next 5 days. By setting the regularization parameter α=0.05, we control the degree of sparsity in the model.\n",
    "\n",
    "This approach is particularly useful for reducing dimensionality and identifying the most relevant features. However, Lasso Regression may struggle with highly correlated variables, arbitrarily selecting one feature over others due to the nature of L1-regularization.\n",
    "\n",
    "\n"
>>>>>>> 7f9679f0b28beed48d0dc9b742bc0bfc4ffac1c0
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed06f14f-ede1-496e-9d99-00682a22bd33",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cf234f-7c2e-498b-ae85-5bc40a0121fd",
   "metadata": {},
   "source": [
    "After training and evaluating the Weighted Average Predictor, we determined the optimal weights to assign to each model in the ensemble. Our final configuration gave full weight to the Ridge Regression model, with a weight of 1.0, and assigned zero weight to the other models (Lasso, Random Forest, and Baselines). This weight configuration was chosen to minimize the Mean Squared Error (MSE), which resulted in an optimal MSE of 49.97.\n",
    "\n",
    "The historical NOAA data, supplemented with current OpenWeather data, served as the foundation for our predictions. For evaluation, we used the period from Nov 25 to Dec 10 of the past five years, ensuring that the evaluation set was representative of real-world forecasting conditions. Our weighted model, which included a variety of predictors such as Linear Regression and Random Forest, provided a robust forecasting tool. The final result was a Ridge Regression model trained on temperature, snowfall, and precipitation from the past 30 days.\n",
    "\n",
    "To ensure the accessibility and reproducibility of our results, we packaged the entire workflow into a Docker image. This step allows anyone interested to easily run our models with minimal setup, ensuring that the results can be verified and applied in different environments. This approach highlights the importance of not only developing accurate models but also making them accessible and reproducible for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51e8507-2662-4d05-a053-a5d3c020c0df",
   "metadata": {},
   "source": [
    "### Further work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94b4943-88fc-4253-bee7-570382f0761f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92d50613-7719-483d-8156-d1be9f6171f8",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946d7870-b546-4c6e-9514-5e3d750163f3",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b89478-8fe0-41b0-9535-1927f8b32902",
   "metadata": {},
   "source": [
    "#### Data Downloading and Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7324680-ee9d-41dc-8d30-18b3b17fb2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading data for 20 cities using NOAA dataset\n",
    "\n",
    "station_code_dict = {\n",
    "    \"PANC\": \"USW00026451\", # Anchorage \n",
    "    \"KBOI\": \"USW00024131\", # Boise  \n",
    "    \"KORD\": \"USW00094846\", # Chicago\n",
    "    \"KDEN\": \"USW00003017\", # Denver \n",
    "    \"KDTW\": \"USW00094847\", # Detroit\n",
    "    \"PHNL\": \"USW00022521\", # Honolulu \n",
    "    \"KIAH\": \"USW00012960\", # Houston\n",
    "    \"KMIA\": \"USW00012839\", # Miami \n",
    "    \"KMSP\": \"USW00014922\", # Minneapolis \n",
    "    \"KOKC\": \"USW00013967\", # Oklahoma City \n",
    "    \"KBNA\": \"USW00013897\", # Nashville \n",
    "    \"KJFK\": \"USW00094789\", # New York \n",
    "    \"KPHX\": \"USW00023183\", # Phoenix \n",
    "    \"KPWM\": \"USW00014764\", # Portland ME\n",
    "    \"KPDX\": \"USW00024229\", # Portland OR \n",
    "    \"KSLC\": \"USW00024127\", # Salt Lake City\n",
    "    \"KSAN\": \"USW00023188\", # San Diego \n",
    "    \"KSFO\": \"USW00023234\", # San Francisco \n",
    "    \"KSEA\": \"USW00024233\", # Seattle \n",
    "    \"KDCA\": \"USW00013743\", # Washington DC\n",
    "}\n",
    "\n",
    "data_path_url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/all/\"\n",
    "\n",
    "# Directory to save downloaded files\n",
    "original_noaa_cache = \"data/original\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(original_noaa_cache, exist_ok=True)\n",
    "\n",
    "# URL to download data from\n",
    "data_path_url = \"https://www.ncei.noaa.gov/pub/data/ghcn/daily/all/\"\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Loop through the station codes and download the corresponding files\n",
    "for station_code, file_name in station_code_dict.items():\n",
    "    url = f\"{data_path_url}{file_name}.dly\"\n",
    "    try:\n",
    "        # Download the file and save it\n",
    "        urllib.request.urlretrieve(url, os.path.join(original_noaa_cache, f\"{station_code}.dly\"))\n",
    "        logging.info(f\"Successfully scraped data for: {station_code}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to download data for {station_code}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e42ad45-bea9-487f-a332-430fcb83d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting NOAA dataset in .csv format\n",
    "\n",
    "# Define the column names and their respective column positions\n",
    "columns = ['ID', 'YEAR', 'MONTH', 'ELEMENT'] + [f'VALUE{i}' for i in range(1, 32)] + [f'MFLAG{i}' for i in range(1, 32)] + [f'QFLAG{i}' for i in range(1, 32)] + [f'SFLAG{i}' for i in range(1, 32)]\n",
    "\n",
    "# Define the fixed column positions for each variable\n",
    "column_positions = [\n",
    "    (0, 11),  # ID (1-11)\n",
    "    (11, 15),  # YEAR (12-15)\n",
    "    (15, 17),  # MONTH (16-17)\n",
    "    (17, 21)  # ELEMENT (18-21)\n",
    "] + [(i*5+21, i*5+26) for i in range(31)] * 3  # VALUE1 to VALUE31, MFLAG1 to MFLAG31, QFLAG1 to QFLAG31, SFLAG1 to SFLAG31\n",
    "\n",
    "\n",
    "# Function to parse a single line of the file\n",
    "def parse_line(line):\n",
    "    data = {}\n",
    "    \n",
    "    # Extract the values for each column based on their positions\n",
    "    data['ID'] = line[0:11].strip()\n",
    "    data['YEAR'] = int(line[11:15].strip())\n",
    "    data['MONTH'] = int(line[15:17].strip())\n",
    "    data['ELEMENT'] = line[17:21].strip()\n",
    "\n",
    "    # Extract VALUE, MFLAG, QFLAG, SFLAG columns\n",
    "    for i in range(31):\n",
    "        start = 21 + i * 8\n",
    "        value_str = line[start:start + 5].strip()\n",
    "        try:\n",
    "            data[f'VALUE{i + 1}'] = int(value_str) if value_str else None\n",
    "        except ValueError:\n",
    "            data[f'VALUE{i + 1}'] = None\n",
    "        data[f'MFLAG{i + 1}'] = line[start + 5:start + 6].strip()\n",
    "        data[f'QFLAG{i + 1}'] = line[start + 6:start + 7].strip()\n",
    "        data[f'SFLAG{i + 1}'] = line[start + 7:start + 8].strip()\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_dly_to_dataframe(input_dir, output_dir, parse_line, file_extension=\"csv\"):\n",
    "    \"\"\"\n",
    "    Converts all .dly files from the input directory to DataFrames and saves them in the output directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create output directory if it doesn't exist\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        if filename.endswith(\".dly\"):\n",
    "            file_path = os.path.join(input_dir, filename)\n",
    "            \n",
    "            # Read and parse the file into a list of dictionaries\n",
    "            records = []\n",
    "            with open(file_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    record = parse_line(line)\n",
    "                    records.append(record)\n",
    "            \n",
    "            # Convert to DataFrame\n",
    "            df = pd.DataFrame(records)\n",
    "            \n",
    "            # Save the DataFrame\n",
    "            output_file_path = os.path.join(output_dir, f\"{os.path.splitext(filename)[0]}.{file_extension}\")\n",
    "            if file_extension == \"csv\":\n",
    "                df.to_csv(output_file_path, index=False)\n",
    "            elif file_extension == \"parquet\":\n",
    "                df.to_parquet(output_file_path, index=False)\n",
    "            \n",
    "            print(f\"Saved {output_file_path}\")\n",
    "\n",
    "\n",
    "input_dir = 'data/original'\n",
    "output_dir = 'data/processed'\n",
    "\n",
    "convert_dly_to_dataframe(input_dir, output_dir, parse_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfea8e9f-314c-4db9-9f65-0b3d3339e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOAA data preprocessing\n",
    "\n",
    "def process_weather_data(data_file, hourly_data_file):\n",
    "    data = pd.read_csv(data_file)\n",
    "    hourly_data = pd.read_csv(hourly_data_file)\n",
    "\n",
    "    # Convert the 'Datetime' column in 'hourly_data' to a datetime object and extract Year, Month, Day\n",
    "    hourly_data['Datetime'] = pd.to_datetime(hourly_data[['Year', 'Month', 'Day']])\n",
    "    hourly_data['Year'] = hourly_data['Datetime'].dt.year\n",
    "    hourly_data['Month'] = hourly_data['Datetime'].dt.month\n",
    "    hourly_data['Day'] = hourly_data['Datetime'].dt.day\n",
    "\n",
    "    data.loc[\n",
    "        (data['YEAR'] == 2024) &\n",
    "        ((data['MONTH'] == 11) & (data['DAY'] >= 12)),\n",
    "        ['TAVG', 'TMIN', 'TMAX', 'PRCP']\n",
    "    ] = None  # Replace values of these columns to NA for this date range\n",
    "\n",
    "    hourly_aggregated = hourly_data.groupby(['Year', 'Month', 'Day']).agg(\n",
    "        TAVG=('Temperature (F)', 'mean'),  # Average Temperature to TAVG\n",
    "        TMIN=('Temp Min (F)', 'min'),  # Min Temp to TMIN\n",
    "        TMAX=('Temp Max (F)', 'max'),  # Max Temp to TMAX\n",
    "        PRCP=('Rain (1h)', 'sum'),  # Pressure to PRCP\n",
    "        SNOW=('Snow (1h)', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Rename the columns in hourly_aggregated to avoid conflict during merge\n",
    "    hourly_aggregated = hourly_aggregated.rename(columns={\n",
    "        'TAVG': 'TAVG_new',\n",
    "        'TMIN': 'TMIN_new',\n",
    "        'TMAX': 'TMAX_new',\n",
    "        'PRCP': 'PRCP_new',\n",
    "        'SNOW': 'SNOW_new'\n",
    "    })\n",
    "\n",
    "    # Merge the aggregated hourly data with 'data', replacing NA values with hourly aggregated values\n",
    "    data = pd.merge(data, hourly_aggregated, how='outer', left_on=['YEAR', 'MONTH', 'DAY'],\n",
    "                    right_on=['Year', 'Month', 'Day'])\n",
    "\n",
    "    # Replace the NA values in the columns with the values from the aggregated hourly\n",
    "    data['YEAR'] = data['YEAR'].combine_first(data['Year']).astype(int)\n",
    "    data['MONTH'] = data['MONTH'].combine_first(data['Month']).astype(int)\n",
    "    data['DAY'] = data['DAY'].combine_first(data['Day']).astype(int)\n",
    "\n",
    "    data['TAVG'] = data['TAVG_new'].combine_first(data['TAVG'])\n",
    "    data['TAVG'] = data['TAVG_new'].combine_first(data['TAVG'])  # Replace NA in 'TAVG' with hourly aggregated value\n",
    "    data['TMIN'] = data['TMIN_new'].combine_first(data['TMIN'])  # Replace NA in 'TMIN' with hourly aggregated value\n",
    "    data['TMAX'] = data['TMAX_new'].combine_first(data['TMAX'])  # Replace NA in 'TMAX' with hourly aggregated value\n",
    "    data['PRCP'] = data['PRCP_new'].combine_first(data['PRCP'])  # Replace NA in 'PRCP' with hourly aggregated value\n",
    "    data['SNOW'] = data['SNOW_new'].combine_first(data['SNOW'])\n",
    "    # Drop the extra columns created during the merge (e.g., columns with '_new' suffix)\n",
    "    data['SNWD'] = data['SNWD'].fillna(0)\n",
    "    data = data.drop(columns=[col for col in data.columns if col.endswith('_new') or col in ['Year', 'Month', 'Day']])\n",
    "\n",
    "    return data\n",
    "\n",
    "def process_all_weather_data(data_directory, hourly_data_directory, locations):\n",
    "    \"\"\"\n",
    "    Process all weather data files in the given directories using the provided location mapping.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the 'combined' directory exists within data_directory\n",
    "    combined_directory = os.path.join(data_directory, 'combined')\n",
    "    os.makedirs(combined_directory, exist_ok=True)  # Create the combined directory if it doesn't exist\n",
    "\n",
    "    # Iterate through each location in the locations dictionary\n",
    "    for city, details in locations.items():\n",
    "        airport_id = details['id']\n",
    "        hourly_data_file = f\"{hourly_data_directory}/{city}_hourly_data.csv\"\n",
    "        data_file = f\"{data_directory}/{airport_id}.csv\"\n",
    "\n",
    "        if os.path.exists(data_file) and os.path.exists(hourly_data_file):\n",
    "            #print(f\"Processing data for {city}...\")\n",
    "\n",
    "            # Process the data using the process_weather_data function\n",
    "            final_data = process_weather_data(data_file, hourly_data_file)\n",
    "\n",
    "            # Save the processed data to the 'combined' directory\n",
    "            output_file = os.path.join(combined_directory, f\"{airport_id}.csv\")\n",
    "            final_data.to_csv(output_file, index=False)\n",
    "            #print(f\"Processed data saved to {output_file}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Files not found for {city}. Skipping...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f40bfc-0713-41f4-9ac5-49f4c9b56f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading data for OpenWeather\n",
    "\n",
    "# Coordinates for each location\n",
    "locations = {\n",
    "    \"Anchorage\": {\"lat\": 61.2181, \"lon\": -149.9003, \"id\": \"PANC\"},\n",
    "    \"Boise\": {\"lat\": 43.6150, \"lon\": -116.2023, \"id\": \"KBOI\"},\n",
    "    \"Chicago\": {\"lat\": 41.8781, \"lon\": -87.6298, \"id\": \"KORD\"},\n",
    "    \"Denver\": {\"lat\": 39.7392, \"lon\": -104.9903, \"id\": \"KDEN\"},\n",
    "    \"Detroit\": {\"lat\": 42.3314, \"lon\": -83.0458, \"id\": \"KDTW\"},\n",
    "    \"Honolulu\": {\"lat\": 21.3069, \"lon\": -157.8583, \"id\": \"PHNL\"},\n",
    "    \"Houston\": {\"lat\": 29.7604, \"lon\": -95.3698, \"id\": \"KIAH\"},\n",
    "    \"Miami\": {\"lat\": 25.7617, \"lon\": -80.1918, \"id\": \"KMIA\"},\n",
    "    \"Minneapolis\": {\"lat\": 44.9778, \"lon\": -93.2650, \"id\": \"KMSP\"},\n",
    "    \"Oklahoma City\": {\"lat\": 35.4676, \"lon\": -97.5164, \"id\": \"KOKC\"},\n",
    "    \"Nashville\": {\"lat\": 36.1627, \"lon\": -86.7816, \"id\": \"KBNA\"},\n",
    "    \"New York\": {\"lat\": 40.7128, \"lon\": -74.0060, \"id\": \"KJFK\"},\n",
    "    \"Phoenix\": {\"lat\": 33.4484, \"lon\": -112.0740, \"id\": \"KPHX\"},\n",
    "    \"Portland ME\": {\"lat\": 43.6591, \"lon\": -70.2568, \"id\": \"KPWM\"},\n",
    "    \"Portland OR\": {\"lat\": 45.5051, \"lon\": -122.6750, \"id\": \"KPDX\"},\n",
    "    \"Salt Lake City\": {\"lat\": 40.7608, \"lon\": -111.8910, \"id\": \"KSLC\"},\n",
    "    \"San Diego\": {\"lat\": 32.7157, \"lon\": -117.1611, \"id\": \"KSAN\"},\n",
    "    \"San Francisco\": {\"lat\": 37.7749, \"lon\": -122.4194, \"id\": \"KSFO\"},\n",
    "    \"Seattle\": {\"lat\": 47.6062, \"lon\": -122.3321, \"id\": \"KSEA\"},\n",
    "    \"Washington DC\": {\"lat\": 38.9072, \"lon\": -77.0369, \"id\": \"KDCA\"}\n",
    "}\n",
    "\n",
    "# Directory where data will be saved\n",
    "repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "directory = os.path.join(repo_root, 'data/original/openweather_hourly/')\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "# Define the time range: past 1 year (in Unix timestamps)\n",
    "end_time = int(time.time())  # Current time in Unix timestamp\n",
    "start_time = end_time - (22 * 24 * 3600)  # One year ago in Unix timestamp\n",
    "\n",
    "\n",
    "# Function to retrieve data for a specific time range\n",
    "def get_data_for_range(city, lat, lon, start_time, end_time):\n",
    "    # Construct the URL for the API call with units=imperial for Fahrenheit\n",
    "    url = f'https://history.openweathermap.org/data/2.5/history/city?lat={lat}&lon={lon}&type=hour&start={start_time}&end={end_time}&units=imperial&appid={API_KEY}'\n",
    "\n",
    "    # Send the API request\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        data_json = response.json()  # If request is successful, return data_json\n",
    "        return data_json\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data: {response.status_code}, {response.text}\")\n",
    "        return None  # Return None if the request failed\n",
    "\n",
    "\n",
    "# Loop through each location and retrieve the historical weather data in chunks\n",
    "for city, coords in locations.items():\n",
    "    lat = coords[\"lat\"]\n",
    "    lon = coords[\"lon\"]\n",
    "\n",
    "    # Initialize an empty list to store the full data for this city\n",
    "    full_data = []\n",
    "\n",
    "    # Define the time range in chunks (1 week at a time)\n",
    "    current_start_time = start_time\n",
    "    while current_start_time < end_time:\n",
    "        current_end_time = min(current_start_time + (7 * 24 * 3600), end_time)  # One week ahead, or until end_time\n",
    "\n",
    "        # Get the data for this chunk\n",
    "        data_json = get_data_for_range(city, lat, lon, current_start_time, current_end_time)\n",
    "\n",
    "        if data_json and 'list' in data_json:\n",
    "            # print(\n",
    "            #    f\"Data retrieval successful for {city} from {time.strftime('%Y-%m-%d', time.gmtime(current_start_time))} to {time.strftime('%Y-%m-%d', time.gmtime(current_end_time))}\")\n",
    "            full_data.append(data_json['list'])  # Append the data chunk to the full data list\n",
    "        else:\n",
    "            print(\n",
    "                f\"Failed to retrieve data for {city} from {time.strftime('%Y-%m-%d', time.gmtime(current_start_time))} to {time.strftime('%Y-%m-%d', time.gmtime(current_end_time))}.\")\n",
    "\n",
    "        # Move the start time forward by one week\n",
    "        current_start_time = current_end_time\n",
    "\n",
    "    # Save the full data as a JSON file for later use\n",
    "    if full_data:\n",
    "        file_name = f'{city}_hourly_data.json'\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "\n",
    "        # Flatten the list of data chunks and save to file\n",
    "        with open(file_path, 'w') as json_file:\n",
    "            json.dump(full_data, json_file, indent=4)\n",
    "        # print(f\"Full data saved to '{file_path}'.\")\n",
    "    else:\n",
    "        print(f\"No data saved for {city}.\")\n",
    "\n",
    "API_KEY = os.getenv('OPENWEATHER_API_KEY')\n",
    "\n",
    "# Dictionary containing city names and their coordinates (latitude, longitude)\n",
    "city_coordinates = {\n",
    "    'Anchorage': (61.2181, -149.9003),\n",
    "    'Boise': (43.615, -116.2023),\n",
    "    'Chicago': (41.8781, -87.6298),\n",
    "    'Denver': (39.7392, -104.9903),\n",
    "    'Detroit': (42.3314, -83.0458),\n",
    "    'Honolulu': (21.3069, -157.8583),\n",
    "    'Houston': (29.7604, -95.3698),\n",
    "    'Miami': (25.7617, -80.1918),\n",
    "    'Minneapolis': (44.9778, -93.265),\n",
    "    'Oklahoma City': (35.4676, -97.5164),\n",
    "    'Nashville': (36.1627, -86.7816),\n",
    "    'New York': (40.7128, -74.006),\n",
    "    'Phoenix': (33.4484, -112.074),\n",
    "    'Portland ME': (43.6591, -70.2568),\n",
    "    'Portland OR': (45.5051, -122.675),\n",
    "    'Salt Lake City': (40.7608, -111.891),\n",
    "    'San Diego': (32.7157, -117.1611),\n",
    "    'San Francisco': (37.7749, -122.4194),\n",
    "    'Seattle': (47.6062, -122.3321),\n",
    "    'Washington DC': (38.9072, -77.0369)\n",
    "}\n",
    "\n",
    "BASE_URL = 'https://api.openweathermap.org/data/2.5/weather'\n",
    "\n",
    "# Directory to save the combined weather data file\n",
    "output_dir = '../data/original'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# CSV file path\n",
    "csv_file_path = os.path.join(output_dir, \"current_weather_data.csv\")\n",
    "\n",
    "# Fetch weather data and save to CSV\n",
    "with open(csv_file_path, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header row\n",
    "    writer.writerow([\n",
    "        \"City\", \"Latitude\", \"Longitude\", \"Weather Description\", \"Temperature (F)\", \"Feels Like (F)\",\n",
    "        \"Temp Min (F)\", \"Temp Max (F)\", \"Pressure\", \"Humidity\", \"Sea Level\", \"Ground Level\",\n",
    "        \"Wind Speed\", \"Wind Deg\", \"Wind Gust\", \"Clouds All\", \"Datetime\", \"Country\",\n",
    "        \"Sunrise\", \"Sunset\", \"Timezone\", \"City ID\", \"City Name\"\n",
    "    ])\n",
    "\n",
    "    # Function to fetch weather data for a city\n",
    "    def fetch_weather(city, lat, lon):\n",
    "        params = {\n",
    "            'lat': lat,\n",
    "            'lon': lon,\n",
    "            'appid': API_KEY,\n",
    "            'units': 'imperial'  # Use 'metric' for Celsius\n",
    "        }\n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "        if response.status_code == 200:\n",
    "            weather_data = response.json()\n",
    "            # Extract relevant data\n",
    "            coord = weather_data.get('coord', {})\n",
    "            weather = weather_data.get('weather', [{}])[0]\n",
    "            main = weather_data.get('main', {})\n",
    "            wind = weather_data.get('wind', {})\n",
    "            clouds = weather_data.get('clouds', {})\n",
    "            sys = weather_data.get('sys', {})\n",
    "\n",
    "            # Create a row with all requested data, handling missing values\n",
    "            return [\n",
    "                city,\n",
    "                coord.get('lat', 'N/A'), coord.get('lon', 'N/A'),\n",
    "                weather.get('description', 'N/A'),\n",
    "                main.get('temp', 'N/A'), main.get('feels_like', 'N/A'),\n",
    "                main.get('temp_min', 'N/A'), main.get('temp_max', 'N/A'),\n",
    "                main.get('pressure', 'N/A'), main.get('humidity', 'N/A'),\n",
    "                main.get('sea_level', 'N/A'), main.get('grnd_level', 'N/A'),\n",
    "                wind.get('speed', 'N/A'), wind.get('deg', 'N/A'), wind.get('gust', 'N/A'),\n",
    "                clouds.get('all', 'N/A'),\n",
    "                weather_data.get('dt', 'N/A'), sys.get('country', 'N/A'),\n",
    "                sys.get('sunrise', 'N/A'), sys.get('sunset', 'N/A'),\n",
    "                weather_data.get('timezone', 'N/A'), weather_data.get('id', 'N/A'),\n",
    "                weather_data.get('name', 'N/A')\n",
    "            ]\n",
    "        else:\n",
    "            print(f\"Error fetching data for {city}. Status code: {response.status_code}\")\n",
    "            return [city] + [\"Error\"] * 21\n",
    "\n",
    "    # Fetch and write weather data for each city\n",
    "    for city, (lat, lon) in city_coordinates.items():\n",
    "        city_weather = fetch_weather(city, lat, lon)\n",
    "        writer.writerow(city_weather)\n",
    "\n",
    "print(f\"Detailed weather data saved to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97946fd9-4039-44d9-b5f4-653e04807517",
   "metadata": {},
   "source": [
    "### Rgression Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b4ad29-44e4-46c3-8cb3-0bfbc35f7429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following functions we create a regression database to train predictors on.\n",
    "# Each row of the database will represent one \"training example\" which consists of:\n",
    "# - The current date\n",
    "# - The station string\n",
    "# - The previous 30 days data of TMIN, TAVG, TMAX, SNOW, PRCP, flattened into a 1D array\n",
    "# - The labels, which next 5 days data of TMIN, TAVG, TMAX, flattened into a 1D array\n",
    "\n",
    "def is_valid_date(year, month, day):\n",
    "  \"\"\"\n",
    "  Checks if a given date is valid.\n",
    "\n",
    "  Args:\n",
    "    year (int): The year.\n",
    "    month (int): The month (1-12).\n",
    "    day (int): The day.\n",
    "\n",
    "  Returns:\n",
    "    bool: True if the date is valid, False otherwise.\n",
    "  \"\"\"\n",
    "\n",
    "  try:\n",
    "    datetime.date(year, month, day)\n",
    "    return True\n",
    "  except ValueError:\n",
    "    return False\n",
    "\n",
    "stations_list = [\n",
    "    \"PANC\", # Anchorage\n",
    "    \"KBOI\", # Boise\n",
    "    \"KORD\", # Chicago\n",
    "    \"KDEN\", # Denver\n",
    "    \"KDTW\", # Detroit\n",
    "    \"PHNL\", # Honolulu\n",
    "    \"KIAH\", # Houston\n",
    "    \"KMIA\", # Miami\n",
    "    \"KMSP\", # Minneapolis\n",
    "    \"KOKC\", # Oklahoma City\n",
    "    \"KBNA\", # Nashville\n",
    "    \"KJFK\", # New York\n",
    "    \"KPHX\", # Phoenix\n",
    "    \"KPWM\", # Portland ME\n",
    "    \"KPDX\", # Portland OR\n",
    "    \"KSLC\", # Salt Lake City\n",
    "    \"KSAN\", # San Diego\n",
    "    \"KSFO\", # San Francisco\n",
    "    \"KSEA\", # Seattle\n",
    "    \"KDCA\", # Washington DC\n",
    "]\n",
    "\n",
    "# Write function that takes in a given day and station, and returns a single row of the regression dataset\n",
    "\n",
    "def create_regression_example(year, month, day, station):\n",
    "    \"\"\"\n",
    "    Create a single row of the regression dataset for a given day and station.\n",
    "    \"\"\"\n",
    "    # Get the data for the specified station and year\n",
    "    repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "    file_path = os.path.join(repo_root, f\"data/restructured_simple/{station}.csv\")\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Find current day index\n",
    "    current_day_index = df[(df['YEAR'] == year) & (df['MONTH'] == month) & (df['DAY'] == day)].index[0]\n",
    "\n",
    "    # Get the previous 30 days data of TMIN, TAVG, TMAX, SNOW, PRCP, flattened into a 1D array\n",
    "    previous_data = df.loc[current_day_index-30:current_day_index-1, ['TMIN', 'TAVG', 'TMAX', 'SNOW', 'PRCP']].values\n",
    "    previous_data = previous_data.flatten()\n",
    "\n",
    "    # Get the labels, which are the next 5 days data of TMIN, TAVG, TMAX, flattened into a 1D array\n",
    "    labels = df.loc[current_day_index:current_day_index+4, ['TMIN', 'TAVG', 'TMAX']].values\n",
    "    labels = labels.flatten()\n",
    "\n",
    "    # Combine all the data into a single row\n",
    "    example = np.concatenate([np.array([year, month, day]), previous_data, labels])\n",
    "\n",
    "    # reshape the array to be (x,1)\n",
    "    #example = example.reshape(-1, 1)\n",
    "\n",
    "    return example\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the regression dataset for a given station\n",
    "# Rules for which current days to include:\n",
    "# - Only use data from 2014 to 2023\n",
    "# - Use all days not including November 15 to December 15\n",
    "# Input: station (string)\n",
    "# Output: dataframe of regression dataset\n",
    "\n",
    "def create_regression_dataset(station):\n",
    "\n",
    "    \"\"\"\n",
    "    Create the regression dataset for a given station.\n",
    "\n",
    "    Args:\n",
    "        station (str): The station for which to create the regression dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The regression dataset for the specified station.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the regression examples\n",
    "    regression_examples = []\n",
    "    # Loop through all days from 2014 to 2023\n",
    "    for year in range(2014, 2025):\n",
    "        for month in range(1, 13):\n",
    "            for day in range(1, 32):\n",
    "\n",
    "                if year == 2024 and month >= 11:\n",
    "                    continue\n",
    "                # Skip days from November 15 to December 15\n",
    "                if month == 11 and day >= 15 and day <= 30:\n",
    "                    continue\n",
    "                if month == 12 and day >= 1 and day <= 15:\n",
    "                    continue\n",
    "                # Skip invalid dates\n",
    "                if not is_valid_date(year, month, day):\n",
    "                    continue\n",
    "\n",
    "                # Create a regression example for the current day\n",
    "                example = create_regression_example(year, month, day, station)\n",
    "                regression_examples.append(example)\n",
    "\n",
    "    # Save the regression examples in a CSV\n",
    "    regression_df = pd.DataFrame(regression_examples)\n",
    "    repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "    path_dir = os.path.join(repo_root, f'analysis/regression_data/{station}.csv')\n",
    "    regression_df.to_csv(path_dir, index=False)\n",
    "\n",
    "\n",
    "\n",
    "def create_regression_dataset_full(station):\n",
    "\n",
    "    \"\"\"\n",
    "    Create the regression dataset for a given station.\n",
    "\n",
    "    Args:\n",
    "        station (str): The station for which to create the regression dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The regression dataset for the specified station.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the regression examples\n",
    "    regression_examples = []\n",
    "    # Loop through all days from 2014 to 2023\n",
    "    for year in range(2014, 2025):\n",
    "        for month in range(1, 13):\n",
    "            for day in range(1, 32):\n",
    "                # Skip invalid dates\n",
    "                if year == 2024 and month >= 11:\n",
    "                    continue\n",
    "\n",
    "                if not is_valid_date(year, month, day):\n",
    "                    continue\n",
    "\n",
    "                # Create a regression example for the current day\n",
    "                example = create_regression_example(year, month, day, station)\n",
    "                regression_examples.append(example)\n",
    "\n",
    "    # Save the regression examples in a CSV\n",
    "    regression_df = pd.DataFrame(regression_examples)\n",
    "    repo_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "    path_dir = os.path.join(repo_root, f'analysis/regression_data_full/{station}.csv')\n",
    "    regression_df.to_csv(path_dir, index=False)\n",
    "\n",
    "# do the same for all stations\n",
    "for station in stations_list:\n",
    "    create_regression_dataset(station)\n",
    "    create_regression_dataset_full(station)\n",
    "    print(f\"Created regression dataset for station {station}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361def81-9e92-4e86-8c61-91381877d51e",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b7b65b-9bcc-415d-baf9-0ea8833a618b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions contain code to evaluate the performance of the model.\n",
    "# The model is evaluated using the mean squared error (MSE)\n",
    "\n",
    "\n",
    "# Helper function to get data for a specific station at a specific year.\n",
    "# Given the year X, we want data from Oct 1, X to Dec. 31, X.\n",
    "# Input: station (string), year (int), directory (string)\n",
    "\n",
    "def get_data_station_year(station, year, directory = \"data/restructured_simple/\"):\n",
    "    # Get the data for the specified station and year\n",
    "    filename = f\"{station}.csv\"\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # The first column of the dataframe is the year, second is the month, third is the day\n",
    "    df = df[((df['YEAR'] == year) & (df['MONTH'] >= 10)) ]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Helper function to predict t_min, t_avg, t_max for the next five days, given the current day of a specific station.\n",
    "# Input: station (string), year (int), month (int), day (int), predictor (Predictor object), data (dataframe)\n",
    "# Output: list of predicted temperatures for the next five days\n",
    "\n",
    "def predict_station_day(station, predictor, data):\n",
    "\n",
    "    predictions = predictor.predict(data, station)\n",
    "    #print(predictions)\n",
    "    return predictions\n",
    "\n",
    "#predict_station_day(\"KBNA\", 2023, 11, 19, predictor.test_predictor.TestPredictor(), None)\n",
    "\n",
    "# Helper function to get MSE for a specific station and current day for predictions of the next five days.\n",
    "# Input: station (string), year (int), month (int), day (int), predictor (Predictor object), data (dataframe)\n",
    "# Output: MSE (float)\n",
    "\n",
    "def get_mse_station_day(station, year, month, day, predictor, data):\n",
    "    # get index of current day\n",
    "    current_day_index = data[(data['YEAR'] == year) & (data['MONTH'] == month) & (data['DAY'] == day)].index[0]\n",
    "    # grab the following five rows using index\n",
    "    actual_temps = data.loc[current_day_index:current_day_index+4, ['TMIN', 'TAVG', 'TMAX']].values\n",
    "    # make into a 1D list\n",
    "    actual_temps = actual_temps.flatten()\n",
    "    actual_temps = actual_temps.tolist()\n",
    "\n",
    "    # grab the data up to the current day\n",
    "    data = data.loc[:current_day_index - 1]\n",
    "\n",
    "    # Get the predicted temperatures for the next five days\n",
    "    predicted_temps = predict_station_day(station, predictor, data)\n",
    "\n",
    "    mse = np.mean((actual_temps - predicted_temps) ** 2)\n",
    "    #print(mse)\n",
    "    return mse\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model on a given station and year\n",
    "# Input: station (string), year (int), predictor (Predictor object)\n",
    "# Output: MSE (float)\n",
    "\n",
    "def evaluate_model_station_year(station, year, predictor):\n",
    "    # Get the data for the specified station and year\n",
    "    df = get_data_station_year(station, year)\n",
    "    # Initialize a list to store the MSE for each day\n",
    "    mses = []\n",
    "    # Iterate over each day in November and December of the given year\n",
    "    for month in range(11, 13):\n",
    "        if month == 11:\n",
    "            day_range = range(25, 31)\n",
    "        else:\n",
    "            day_range = range(1, 11)\n",
    "        for day in day_range:\n",
    "            # Calculate the MSE for the current day\n",
    "            mse = get_mse_station_day(station, year, month, day, predictor, df)\n",
    "            #print(mse)\n",
    "            mses.append(mse)\n",
    "    # Calculate the average MSE for the year\n",
    "    avg_mse = np.mean(mses)\n",
    "    return avg_mse\n",
    "\n",
    "\n",
    "# Function to evaluate for all stations in a given year\n",
    "# Input: year (int), predictor (Predictor object)\n",
    "# Output: Mean MSE (float)\n",
    "\n",
    "def evaluate_model_year(year, predictor):\n",
    "    # Initialize a list to store the MSE for each station\n",
    "    mse_list = []\n",
    "    # Iterate over each station\n",
    "    for station in stations_list:\n",
    "        # Calculate the MSE for the station and year\n",
    "        mse = evaluate_model_station_year(station, year, predictor)\n",
    "        #print(station)\n",
    "        #print(mse)\n",
    "        mse_list.append(mse)\n",
    "    # print(mse_list)\n",
    "    # Calculate the mean MSE for all stations\n",
    "    mean_mse = np.mean(mse_list)\n",
    "    return mean_mse\n",
    "\n",
    "# evaluate_model_year(2019, predictor.test_predictor.LinearRegressionPredictor())\n",
    "\n",
    "# Function to evaluate the model for a given amount of years\n",
    "# Input: start_year (int), end_year (int), predictor (Predictor object)\n",
    "# Output: List of mean MSE for each year\n",
    "\n",
    "def evaluate_model_years(start_year, end_year, predictor):\n",
    "    mse_list = []\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        mse = evaluate_model_year(year, predictor)\n",
    "        mse_list.append(mse)\n",
    "        # print(mse)\n",
    "    # make mse_list into python list\n",
    "    # mse_list = mse_list.tolist()\n",
    "\n",
    "    # round to 2 decimal places\n",
    "    mse_list = np.round(mse_list, 2)\n",
    "    return mse_list\n",
    "\n",
    "# function to evaluate all models for a given amount of years\n",
    "def eval_all_models_years(models, start_year, end_year):\n",
    "    # create a list of all models in predictor.test_predictor\n",
    "\n",
    "\n",
    "    # print the model name and the mean mse for each year\n",
    "    for model in models:\n",
    "        print(str(model))\n",
    "        print(evaluate_model_years(start_year, end_year, model).round(2))\n",
    "\n",
    "# define a function that takes in a list of predictors and a year range, and returns the mean mse across the years,\n",
    "# for different weights of the models with WeightedPredictor\n",
    "\n",
    "def testing_weight_predictor(predictor_list, start_year, end_year, iters):\n",
    "    # get number of predictors\n",
    "    num_predictors = len(predictor_list)\n",
    "    # create a list of a list of weights to iterate through. Each row is a different set of weights,\n",
    "    # must have num_predictor floats, and sum to 1.\n",
    "    weights_list = np.random.dirichlet(np.ones(num_predictors), size=iters)\n",
    "\n",
    "    # create a list to store the mean mse for each set of weights\n",
    "    mse_list = []\n",
    "\n",
    "    # iterate through each set of weights\n",
    "    for weights in weights_list:\n",
    "        print(\"once\")\n",
    "\n",
    "        # create a WeightedPredictor object with the given weights\n",
    "        model = predictor.test_predictor.WeightedPredictor(predictor_list, weights)\n",
    "        # get the mean mse for the given set of weights\n",
    "        mse = evaluate_model_years(start_year, end_year, model)\n",
    "        # find mean of mse\n",
    "        mse = np.mean(mse)\n",
    "        # add to mse_list\n",
    "        mse_list.append(mse)\n",
    "\n",
    "    # combine the weights list with the mse list, and sort by mse lowest to highest\n",
    "    mse_list = np.array(mse_list)\n",
    "    weights_list = np.round(weights_list, 2)\n",
    "    mse_list = np.round(mse_list, 2)\n",
    "    combined = np.column_stack((weights_list, mse_list))\n",
    "    combined = combined[combined[:,num_predictors].argsort()]\n",
    "\n",
    "    # save combined to a csv\n",
    "    np.savetxt(\"predictor/weights/weight_mse.csv\", combined, delimiter=\",\", fmt='%s')\n",
    "\n",
    "    return combined\n",
    "\n",
    "\n",
    "\n",
    "predictor_list = [ #predictor.test_predictor.PreviousDayPredictor(),\n",
    "                  predictor.test_predictor.LinearRegressionPredictor(),\n",
    "                  predictor.test_predictor.RidgeRegressionPredictor(),\n",
    "                  predictor.test_predictor.LassoPredictor(),\n",
    "                  predictor.test_predictor.RandomForestPredictor()\n",
    "                  ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c2776e-1e2a-4fc5-8e1d-c8876378926c",
   "metadata": {},
   "source": [
    "### Prediction Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa580bdf-012b-4004-889b-3688919120a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a25a0a-050b-4b39-8c8e-427ec0793c20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
