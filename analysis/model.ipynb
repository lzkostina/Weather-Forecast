{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to ensure that there is a sufficient number of data sets in cross -verification, we choose to use 80%of the samples as a training set and 20%of the samples as tests. The data we use comes from api.openweathermap.org and passed through data cleaning, but for vacant values, due to the increase in extreme weather such as global warming, we adopt different filling methods for different models (in detail in the model part) Essence Our goal is the temperature of the temperature [minimum value, maximum, maximum] five days of prediction value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Historical mean model: Simple, easy to implement time sequence prediction method. As a benchmark model to evaluate the performance of other complex models, its predictive effect will decrease significantly when the data has dynamic changes (such as trends or cyclical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed KBNA, results appended to output/AllCities_HistoricalMean.txt\n",
      "Processed KBOI, results appended to output/AllCities_HistoricalMean.txt\n",
      "Processed KDCA, results appended to output/AllCities_HistoricalMean.txt\n",
      "Processed KDEN, results appended to output/AllCities_HistoricalMean.txt\n",
      "Processed KDTW, results appended to output/AllCities_HistoricalMean.txt\n",
      "Processed KIAH, results appended to output/AllCities_HistoricalMean.txt\n",
      "Processed KJFK, results appended to output/AllCities_HistoricalMean.txt\n",
      "Processed KMIA, results appended to output/AllCities_HistoricalMean.txt\n",
      "Processed KMSP, results appended to output/AllCities_HistoricalMean.txt\n",
      "Processed KOKC, results appended to output/AllCities_HistoricalMean.txt\n",
      "Processed KORD, results appended to output/AllCities_HistoricalMean.txt\n",
      "Processed KPDX, results appended to output/AllCities_HistoricalMean.txt\n",
      "Processed KPHX, results appended to output/AllCities_HistoricalMean.txt\n",
      "Processed KPWM, results appended to output/AllCities_HistoricalMean.txt\n",
      "Processed KSAN, results appended to output/AllCities_HistoricalMean.txt\n",
      "Processed KSEA, results appended to output/AllCities_HistoricalMean.txt\n",
      "Processed KSFO, results appended to output/AllCities_HistoricalMean.txt\n",
      "Processed KSLC, results appended to output/AllCities_HistoricalMean.txt\n",
      "Processed PANC, results appended to output/AllCities_HistoricalMean.txt\n",
      "Processed PHNL, results appended to output/AllCities_HistoricalMean.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def evaluate_predictions(actual, predicted, num_features=1):\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    r2 = r2_score(actual, predicted)  # R² calculation\n",
    "    n = len(actual)  # Number of data points\n",
    "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - num_features - 1)  # Adjusted R²\n",
    "    return mse, mae, r2, adj_r2\n",
    "\n",
    "# Define Historical Mean prediction function\n",
    "def historical_mean_forecast(series, steps=5, start_date=None):\n",
    "    mean_value = series.mean()\n",
    "    forecast_values = [mean_value] * steps\n",
    "    future_dates = [start_date + pd.Timedelta(days=i) for i in range(1, steps + 1)]\n",
    "    forecast_df = pd.DataFrame({'DATE': future_dates, 'FORECAST': forecast_values})\n",
    "    return forecast_df\n",
    "\n",
    "\n",
    "directory = '.'  \n",
    "output_file = 'output/AllCities_HistoricalMean.txt'  \n",
    "os.makedirs('output', exist_ok=True)  \n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"Historical Mean Results for All Cities:\\n\\n\")\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        city_name = os.path.splitext(filename)[0]\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        df['DATE'] = pd.to_datetime(df[['YEAR', 'MONTH', 'DAY']], errors='coerce')\n",
    "        df = df[df['DATE'] <= '2024-11-19']  # Filter data before or on 2024-11-19\n",
    "        df['TAVG'] = (df['TMAX'] + df['TMIN']) / 2  \n",
    "        df['TMAX'] = df['TMAX'].interpolate(method='linear')\n",
    "        df['TMIN'] = df['TMIN'].interpolate(method='linear')\n",
    "        df['TAVG'] = df['TAVG'].interpolate(method='linear')\n",
    "\n",
    "        # Train-Test Split\n",
    "        train_tmax = df['TMAX'][:-5]\n",
    "        test_tmax = df['TMAX'][-5:]\n",
    "        train_tmin = df['TMIN'][:-5]\n",
    "        test_tmin = df['TMIN'][-5:]\n",
    "        train_tavg = df['TAVG'][:-5]\n",
    "        test_tavg = df['TAVG'][-5:]\n",
    "\n",
    "        start_date = df['DATE'].max()  # Start date for predictions\n",
    "        tmax_historical_forecast_df = historical_mean_forecast(train_tmax, steps=5, start_date=start_date)\n",
    "        tmin_historical_forecast_df = historical_mean_forecast(train_tmin, steps=5, start_date=start_date)\n",
    "        tavg_historical_forecast_df = historical_mean_forecast(train_tavg, steps=5, start_date=start_date)\n",
    "\n",
    "        tmax_mse, tmax_mae, tmax_r2, tmax_adj_r2 = evaluate_predictions(test_tmax, tmax_historical_forecast_df['FORECAST'])\n",
    "        tmin_mse, tmin_mae, tmin_r2, tmin_adj_r2 = evaluate_predictions(test_tmin, tmin_historical_forecast_df['FORECAST'])\n",
    "        tavg_mse, tavg_mae, tavg_r2, tavg_adj_r2 = evaluate_predictions(test_tavg, tavg_historical_forecast_df['FORECAST'])\n",
    "\n",
    "        combined_forecast_df = pd.DataFrame({\n",
    "            'DATE': tmax_historical_forecast_df['DATE'],\n",
    "            'TMIN': tmin_historical_forecast_df['FORECAST'],\n",
    "            'TMAX': tmax_historical_forecast_df['FORECAST'],\n",
    "            'TAVG': tavg_historical_forecast_df['FORECAST']\n",
    "        })\n",
    "\n",
    "        output_text = f\"City: {city_name}\\n\"\n",
    "        output_text += f\"TMAX - MSE: {tmax_mse:.2f}, MAE: {tmax_mae:.2f}, R²: {tmax_r2:.4f}, Adjusted R²: {tmax_adj_r2:.4f}\\n\"\n",
    "        output_text += f\"TMIN - MSE: {tmin_mse:.2f}, MAE: {tmin_mae:.2f}, R²: {tmin_r2:.4f}, Adjusted R²: {tmin_adj_r2:.4f}\\n\"\n",
    "        output_text += f\"TAVG - MSE: {tavg_mse:.2f}, MAE: {tavg_mae:.2f}, R²: {tavg_r2:.4f}, Adjusted R²: {tavg_adj_r2:.4f}\\n\\n\"\n",
    "        output_text += \"Future 5-Day Predictions (Combined):\\n\"\n",
    "        output_text += combined_forecast_df.to_string(index=False)\n",
    "        output_text += \"\\n\\n\" + \"-\" * 50 + \"\\n\\n\"\n",
    "\n",
    "        with open(output_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(output_text)\n",
    "\n",
    "        print(f\"Processed {city_name}, results appended to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.OLS model: For the return of OLS to join the lagging project, we consider using the average value instead of FFILL or BFILL is to avoid the effect of extreme changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed KBNA, results appended to output/AllCities_LinearRegression.txt\n",
      "Processed KBOI, results appended to output/AllCities_LinearRegression.txt\n",
      "Processed KDCA, results appended to output/AllCities_LinearRegression.txt\n",
      "Processed KDEN, results appended to output/AllCities_LinearRegression.txt\n",
      "Processed KDTW, results appended to output/AllCities_LinearRegression.txt\n",
      "Processed KIAH, results appended to output/AllCities_LinearRegression.txt\n",
      "Processed KJFK, results appended to output/AllCities_LinearRegression.txt\n",
      "Processed KMIA, results appended to output/AllCities_LinearRegression.txt\n",
      "Processed KMSP, results appended to output/AllCities_LinearRegression.txt\n",
      "Processed KOKC, results appended to output/AllCities_LinearRegression.txt\n",
      "Processed KORD, results appended to output/AllCities_LinearRegression.txt\n",
      "Processed KPDX, results appended to output/AllCities_LinearRegression.txt\n",
      "Processed KPHX, results appended to output/AllCities_LinearRegression.txt\n",
      "Processed KPWM, results appended to output/AllCities_LinearRegression.txt\n",
      "Processed KSAN, results appended to output/AllCities_LinearRegression.txt\n",
      "Processed KSEA, results appended to output/AllCities_LinearRegression.txt\n",
      "Processed KSFO, results appended to output/AllCities_LinearRegression.txt\n",
      "Processed KSLC, results appended to output/AllCities_LinearRegression.txt\n",
      "Processed PANC, results appended to output/AllCities_LinearRegression.txt\n",
      "Processed PHNL, results appended to output/AllCities_LinearRegression.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def evaluate_model(y_test, y_pred, X_test):\n",
    "    n = len(y_test)  # Number of observations\n",
    "    p = X_test.shape[1]  # Number of predictors\n",
    "    r2 = r2_score(y_test, y_pred)  # R² score\n",
    "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)  # Adjusted R²\n",
    "    mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
    "    rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "    mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error\n",
    "    return r2, adj_r2, mse, rmse, mae\n",
    "\n",
    "def generate_future_predictions(data, model_min, model_max, model_avg, days=5, start_date=None, lag_features=[]):\n",
    "    future_predictions = []\n",
    "    current_data = data.iloc[-1][lag_features].values.reshape(1, -1)\n",
    "    current_date = pd.to_datetime(start_date)\n",
    "\n",
    "    for day in range(1, days + 1):\n",
    "        pred_min = model_min.predict(current_data)[0]\n",
    "        pred_max = model_max.predict(current_data)[0]\n",
    "        pred_avg = model_avg.predict(current_data)[0]\n",
    "\n",
    "        future_predictions.append({\n",
    "            'DATE': current_date + pd.Timedelta(days=day),\n",
    "            'TMIN': pred_min,\n",
    "            'TMAX': pred_max,\n",
    "            'TAVG': pred_avg\n",
    "        })\n",
    "\n",
    "        current_data = np.roll(current_data, -3)  \n",
    "        current_data[0, :3] = [pred_max, pred_min, pred_avg]  # Insert new predictions\n",
    "\n",
    "    return pd.DataFrame(future_predictions)\n",
    "\n",
    "\n",
    "directory = '.'  \n",
    "output_file = 'output/AllCities_LinearRegression.txt'  \n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"Linear Regression Model Results for All Cities:\\n\\n\")\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        city_name = os.path.splitext(filename)[0]\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        df['DATE'] = pd.to_datetime(df[['YEAR', 'MONTH', 'DAY']], errors='coerce')  # Handle invalid dates\n",
    "        df = df[df['DATE'].notna()]  \n",
    "        df = df[df['DATE'] <= '2024-11-19']  # Filter data before or on 2024-11-19\n",
    "        df['TMAX'] = df['TMAX'].interpolate(method='linear')\n",
    "        df['TMIN'] = df['TMIN'].interpolate(method='linear')\n",
    "        df['PRCP'] = df['PRCP'].interpolate(method='linear')\n",
    "        df['SNOW'] = df['SNOW'].interpolate(method='linear')\n",
    "        df['SNWD'] = df['SNWD'].interpolate(method='linear')\n",
    "        df['TAVG'] = (df['TMAX'] + df['TMIN']) / 2\n",
    "\n",
    "        for i in range(1, 10):\n",
    "            df[f'TMAX_lag{i}'] = df['TMAX'].shift(i)\n",
    "            df[f'TMIN_lag{i}'] = df['TMIN'].shift(i)\n",
    "            df[f'TAVG_lag{i}'] = df['TAVG'].shift(i)\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "        lag_features = [f'TMAX_lag{i}' for i in range(1, 10)] + \\\n",
    "                       [f'TMIN_lag{i}' for i in range(1, 10)] + \\\n",
    "                       [f'TAVG_lag{i}' for i in range(1, 10)] + ['PRCP', 'SNOW', 'SNWD']\n",
    "\n",
    "        X = df[lag_features]\n",
    "        y_min = df['TMIN']\n",
    "        y_max = df['TMAX']\n",
    "        y_avg = df['TAVG']\n",
    "\n",
    "        # Train-Test Split\n",
    "        X_train, X_test, y_min_train, y_min_test, y_max_train, y_max_test, y_avg_train, y_avg_test = train_test_split(\n",
    "            X, y_min, y_max, y_avg, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        model_min = LinearRegression()\n",
    "        model_max = LinearRegression()\n",
    "        model_avg = LinearRegression()\n",
    "\n",
    "        model_min.fit(X_train, y_min_train)\n",
    "        model_max.fit(X_train, y_max_train)\n",
    "        model_avg.fit(X_train, y_avg_train)\n",
    "\n",
    "        y_min_pred = model_min.predict(X_test)\n",
    "        y_max_pred = model_max.predict(X_test)\n",
    "        y_avg_pred = model_avg.predict(X_test)\n",
    "\n",
    "        r2_min, adj_r2_min, mse_min, rmse_min, mae_min = evaluate_model(y_min_test, y_min_pred, X_test)\n",
    "        r2_max, adj_r2_max, mse_max, rmse_max, mae_max = evaluate_model(y_max_test, y_max_pred, X_test)\n",
    "        r2_avg, adj_r2_avg, mse_avg, rmse_avg, mae_avg = evaluate_model(y_avg_test, y_avg_pred, X_test)\n",
    "\n",
    "        future_predictions = generate_future_predictions(df, model_min, model_max, model_avg, days=5, start_date='2024-11-19', lag_features=lag_features)\n",
    "\n",
    "        output_text = f\"City: {city_name}\\n\"\n",
    "        output_text += f\"TMIN Evaluation: R²: {r2_min:.4f}, Adjusted R²: {adj_r2_min:.4f}, MSE: {mse_min:.4f}, RMSE: {rmse_min:.4f}, MAE: {mae_min:.4f}\\n\"\n",
    "        output_text += f\"TMAX Evaluation: R²: {r2_max:.4f}, Adjusted R²: {adj_r2_max:.4f}, MSE: {mse_max:.4f}, RMSE: {rmse_max:.4f}, MAE: {mae_max:.4f}\\n\"\n",
    "        output_text += f\"TAVG Evaluation: R²: {r2_avg:.4f}, Adjusted R²: {adj_r2_avg:.4f}, MSE: {mse_avg:.4f}, RMSE: {rmse_avg:.4f}, MAE: {mae_avg:.4f}\\n\\n\"\n",
    "        output_text += \"Future 5-Day Predictions (Combined):\\n\"\n",
    "        output_text += future_predictions.to_string(index=False)\n",
    "        output_text += \"\\n\\n\" + \"-\" * 50 + \"\\n\\n\"\n",
    "\n",
    "        with open(output_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(output_text)\n",
    "\n",
    "        print(f\"Processed {city_name}, results appended to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Random Forest: Random Forest Return is suitable for time sequence prediction. It can handle complex non -linear relationships, and there are less distribution of data, but we still need to deal with missing values. We choose linear filling.\n",
    "In order to ensure that the computing time complexity does not exceed requirements, we simplify the random forest and control the number of decision -making trees and the maximum depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed KBNA, results appended to output/AllCities_RandomForest.txt\n",
      "Processed KBOI, results appended to output/AllCities_RandomForest.txt\n",
      "Processed KDCA, results appended to output/AllCities_RandomForest.txt\n",
      "Processed KDEN, results appended to output/AllCities_RandomForest.txt\n",
      "Processed KDTW, results appended to output/AllCities_RandomForest.txt\n",
      "Processed KIAH, results appended to output/AllCities_RandomForest.txt\n",
      "Processed KJFK, results appended to output/AllCities_RandomForest.txt\n",
      "Processed KMIA, results appended to output/AllCities_RandomForest.txt\n",
      "Processed KMSP, results appended to output/AllCities_RandomForest.txt\n",
      "Processed KOKC, results appended to output/AllCities_RandomForest.txt\n",
      "Processed KORD, results appended to output/AllCities_RandomForest.txt\n",
      "Processed KPDX, results appended to output/AllCities_RandomForest.txt\n",
      "Processed KPHX, results appended to output/AllCities_RandomForest.txt\n",
      "Processed KPWM, results appended to output/AllCities_RandomForest.txt\n",
      "Processed KSAN, results appended to output/AllCities_RandomForest.txt\n",
      "Processed KSEA, results appended to output/AllCities_RandomForest.txt\n",
      "Processed KSFO, results appended to output/AllCities_RandomForest.txt\n",
      "Processed KSLC, results appended to output/AllCities_RandomForest.txt\n",
      "Processed PANC, results appended to output/AllCities_RandomForest.txt\n",
      "Processed PHNL, results appended to output/AllCities_RandomForest.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def evaluate_model(y_test, y_pred, X_test):\n",
    "    n = len(y_test)  # Number of observations\n",
    "    p = X_test.shape[1]  # Number of predictors\n",
    "    r2 = r2_score(y_test, y_pred)  # R² score\n",
    "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)  # Adjusted R²\n",
    "    mse = mean_squared_error(y_test, y_pred)  # Mean Squared Error\n",
    "    rmse = np.sqrt(mse)  # Root Mean Squared Error\n",
    "    mae = mean_absolute_error(y_test, y_pred)  # Mean Absolute Error\n",
    "    return r2, adj_r2, mse, rmse, mae\n",
    "\n",
    "def generate_future_predictions(data, model_min, model_max, model_avg, days=5, start_date=None, lag_features=[]):\n",
    "    future_predictions = []\n",
    "    current_data = data.iloc[-1][lag_features].values.reshape(1, -1)\n",
    "    current_date = pd.to_datetime(start_date)\n",
    "\n",
    "    for day in range(1, days + 1):\n",
    "        pred_min = model_min.predict(current_data)[0]\n",
    "        pred_max = model_max.predict(current_data)[0]\n",
    "        pred_avg = model_avg.predict(current_data)[0]\n",
    "\n",
    "        future_predictions.append({\n",
    "            'DATE': current_date + pd.Timedelta(days=day),\n",
    "            'TMIN': pred_min,\n",
    "            'TMAX': pred_max,\n",
    "            'TAVG': pred_avg\n",
    "        })\n",
    "\n",
    "        current_data = np.roll(current_data, -3)  \n",
    "        current_data[0, :3] = [pred_max, pred_min, pred_avg]  # Insert new predictions\n",
    "\n",
    "    return pd.DataFrame(future_predictions)\n",
    "\n",
    "directory = '.' \n",
    "output_file = 'output/AllCities_RandomForest.txt' \n",
    "os.makedirs('output', exist_ok=True) \n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"Random Forest Regressor Results for All Cities:\\n\\n\")\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        city_name = os.path.splitext(filename)[0]\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        df['DATE'] = pd.to_datetime(df[['YEAR', 'MONTH', 'DAY']], errors='coerce')  # Handle invalid dates\n",
    "        df = df[df['DATE'].notna()]  \n",
    "        df = df[df['DATE'] <= '2024-11-19']  # Filter data before or on 2024-11-19\n",
    "\n",
    "        df['TMAX'] = df['TMAX'].interpolate(method='linear')\n",
    "        df['TMIN'] = df['TMIN'].interpolate(method='linear')\n",
    "        df['PRCP'] = df['PRCP'].interpolate(method='linear')\n",
    "        df['SNOW'] = df['SNOW'].interpolate(method='linear')\n",
    "        df['SNWD'] = df['SNWD'].interpolate(method='linear')\n",
    "        df['TAVG'] = (df['TMAX'] + df['TMIN']) / 2\n",
    "\n",
    "        for i in range(1, 6):\n",
    "            df[f'TMAX_lag{i}'] = df['TMAX'].shift(i)\n",
    "            df[f'TMIN_lag{i}'] = df['TMIN'].shift(i)\n",
    "            df[f'TAVG_lag{i}'] = df['TAVG'].shift(i)\n",
    "        df.dropna(inplace=True)\n",
    "        lag_features = [f'TMAX_lag{i}' for i in range(1, 6)] + \\\n",
    "                       [f'TMIN_lag{i}' for i in range(1, 6)] + \\\n",
    "                       [f'TAVG_lag{i}' for i in range(1, 6)] + ['PRCP', 'SNOW', 'SNWD']\n",
    "\n",
    "        X = df[lag_features]\n",
    "        y_min = df['TMIN']\n",
    "        y_max = df['TMAX']\n",
    "        y_avg = df['TAVG']\n",
    "\n",
    "        # Train-Test Split\n",
    "        X_train, X_test, y_min_train, y_min_test, y_max_train, y_max_test, y_avg_train, y_avg_test = train_test_split(\n",
    "            X, y_min, y_max, y_avg, test_size=0.2, random_state=42\n",
    "        )\n",
    "\n",
    "        model_min = RandomForestRegressor(n_estimators=30, max_depth=10,random_state=42)\n",
    "        model_max = RandomForestRegressor(n_estimators=30, max_depth=10,random_state=42)\n",
    "        model_avg = RandomForestRegressor(n_estimators=30, max_depth=10,random_state=42)\n",
    "\n",
    "        model_min.fit(X_train, y_min_train)\n",
    "        model_max.fit(X_train, y_max_train)\n",
    "        model_avg.fit(X_train, y_avg_train)\n",
    "\n",
    "        y_min_pred = model_min.predict(X_test)\n",
    "        y_max_pred = model_max.predict(X_test)\n",
    "        y_avg_pred = model_avg.predict(X_test)\n",
    "\n",
    "        r2_min, adj_r2_min, mse_min, rmse_min, mae_min = evaluate_model(y_min_test, y_min_pred, X_test)\n",
    "        r2_max, adj_r2_max, mse_max, rmse_max, mae_max = evaluate_model(y_max_test, y_max_pred, X_test)\n",
    "        r2_avg, adj_r2_avg, mse_avg, rmse_avg, mae_avg = evaluate_model(y_avg_test, y_avg_pred, X_test)\n",
    "\n",
    "        future_predictions = generate_future_predictions(df, model_min, model_max, model_avg, days=5, start_date='2024-11-19', lag_features=lag_features)\n",
    "\n",
    "        output_text = f\"City: {city_name}\\n\"\n",
    "        output_text += f\"TMIN Evaluation: R²: {r2_min:.4f}, Adjusted R²: {adj_r2_min:.4f}, MSE: {mse_min:.4f}, RMSE: {rmse_min:.4f}, MAE: {mae_min:.4f}\\n\"\n",
    "        output_text += f\"TMAX Evaluation: R²: {r2_max:.4f}, Adjusted R²: {adj_r2_max:.4f}, MSE: {mse_max:.4f}, RMSE: {rmse_max:.4f}, MAE: {mae_max:.4f}\\n\"\n",
    "        output_text += f\"TAVG Evaluation: R²: {r2_avg:.4f}, Adjusted R²: {adj_r2_avg:.4f}, MSE: {mse_avg:.4f}, RMSE: {rmse_avg:.4f}, MAE: {mae_avg:.4f}\\n\\n\"\n",
    "        output_text += \"Future 5-Day Predictions:\\n\"\n",
    "        output_text += future_predictions.to_string(index=False)\n",
    "        output_text += \"\\n\\n\" + \"-\" * 50 + \"\\n\\n\"\n",
    "\n",
    "        with open(output_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(output_text)\n",
    "\n",
    "        print(f\"Processed {city_name}, results appended to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. XGBOOST: The characteristic of XGB is that the training speed is faster. The delayed value does not require additional filling to prevent strong fitting ability, but it is more sensitive to noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed KBNA, results appended to output/AllCities_XGBoost.txt\n",
      "Processed KBOI, results appended to output/AllCities_XGBoost.txt\n",
      "Processed KDCA, results appended to output/AllCities_XGBoost.txt\n",
      "Processed KDEN, results appended to output/AllCities_XGBoost.txt\n",
      "Processed KDTW, results appended to output/AllCities_XGBoost.txt\n",
      "Processed KIAH, results appended to output/AllCities_XGBoost.txt\n",
      "Processed KJFK, results appended to output/AllCities_XGBoost.txt\n",
      "Processed KMIA, results appended to output/AllCities_XGBoost.txt\n",
      "Processed KMSP, results appended to output/AllCities_XGBoost.txt\n",
      "Processed KOKC, results appended to output/AllCities_XGBoost.txt\n",
      "Processed KORD, results appended to output/AllCities_XGBoost.txt\n",
      "Processed KPDX, results appended to output/AllCities_XGBoost.txt\n",
      "Processed KPHX, results appended to output/AllCities_XGBoost.txt\n",
      "Processed KPWM, results appended to output/AllCities_XGBoost.txt\n",
      "Processed KSAN, results appended to output/AllCities_XGBoost.txt\n",
      "Processed KSEA, results appended to output/AllCities_XGBoost.txt\n",
      "Processed KSFO, results appended to output/AllCities_XGBoost.txt\n",
      "Processed KSLC, results appended to output/AllCities_XGBoost.txt\n",
      "Processed PANC, results appended to output/AllCities_XGBoost.txt\n",
      "Processed PHNL, results appended to output/AllCities_XGBoost.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def evaluate_model(y_test, y_pred, X_test):\n",
    "    n = len(y_test)\n",
    "    p = X_test.shape[1]\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    return r2, adj_r2, mse, rmse\n",
    "\n",
    "def generate_future_predictions(data, model_min, model_max, model_avg, days=5, start_date=None, lag_features=[]):\n",
    "    future_predictions = []\n",
    "    current_data = data.iloc[-1][lag_features].values.reshape(1, -1)\n",
    "    current_date = pd.to_datetime(start_date)\n",
    "\n",
    "    for day in range(1, days + 1):\n",
    "        pred_min = model_min.predict(current_data)[0]\n",
    "        pred_max = model_max.predict(current_data)[0]\n",
    "        pred_avg = model_avg.predict(current_data)[0]\n",
    "\n",
    "        future_predictions.append({\n",
    "            'DATE': current_date + pd.Timedelta(days=day),\n",
    "            'TMIN': pred_min,\n",
    "            'TMAX': pred_max,\n",
    "            'TAVG': pred_avg\n",
    "        })\n",
    "\n",
    "        current_data = np.roll(current_data, -3)  \n",
    "        current_data[0, :3] = [pred_max, pred_min, pred_avg]  # Insert new predictions\n",
    "\n",
    "    return pd.DataFrame(future_predictions)\n",
    "\n",
    "directory = '.'  \n",
    "output_file = 'output/AllCities_XGBoost.txt' \n",
    "os.makedirs('output', exist_ok=True)  \n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"XGBoost Model Results for All Cities:\\n\\n\")\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        city_name = os.path.splitext(filename)[0]\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        df['DATE'] = pd.to_datetime(df[['YEAR', 'MONTH', 'DAY']], errors='coerce')\n",
    "        df = df[df['DATE'] <= '2024-11-19']  # Filter data before or on 2024-11-19\n",
    "\n",
    "        df['TAVG'] = (df['TMAX'] + df['TMIN']) / 2\n",
    "        for i in range(1, 6):\n",
    "            df[f'TMAX_lag{i}'] = df['TMAX'].shift(i)\n",
    "            df[f'TMIN_lag{i}'] = df['TMIN'].shift(i)\n",
    "            df[f'TAVG_lag{i}'] = df['TAVG'].shift(i)\n",
    "\n",
    "        df.dropna(subset=['TMAX', 'TMIN', 'TAVG'], inplace=True)\n",
    "\n",
    "        lag_features = [f'TMAX_lag{i}' for i in range(1, 6)] + \\\n",
    "                       [f'TMIN_lag{i}' for i in range(1, 6)] + \\\n",
    "                       [f'TAVG_lag{i}' for i in range(1, 6)] + ['PRCP', 'SNOW', 'SNWD']\n",
    "\n",
    "        X = df[lag_features]\n",
    "        y_min = df['TMIN']\n",
    "        y_max = df['TMAX']\n",
    "        y_avg = df['TAVG']\n",
    "\n",
    "        # Train-Test Split\n",
    "        X_train, X_test, y_min_train, y_min_test, y_max_train, y_max_test, y_avg_train, y_avg_test = train_test_split(\n",
    "            X, y_min, y_max, y_avg, test_size=0.2, random_state=42\n",
    "        )\n",
    "        model_min = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6,\n",
    "                                 subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
    "        model_max = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6,\n",
    "                                 subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
    "        model_avg = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=6,\n",
    "                                 subsample=0.8, colsample_bytree=0.8, random_state=42)\n",
    "\n",
    "        model_min.fit(X_train, y_min_train)\n",
    "        model_max.fit(X_train, y_max_train)\n",
    "        model_avg.fit(X_train, y_avg_train)\n",
    "\n",
    "        y_min_pred = model_min.predict(X_test)\n",
    "        y_max_pred = model_max.predict(X_test)\n",
    "        y_avg_pred = model_avg.predict(X_test)\n",
    "\n",
    "        r2_min, adj_r2_min, mse_min, rmse_min = evaluate_model(y_min_test, y_min_pred, X_test)\n",
    "        r2_max, adj_r2_max, mse_max, rmse_max = evaluate_model(y_max_test, y_max_pred, X_test)\n",
    "        r2_avg, adj_r2_avg, mse_avg, rmse_avg = evaluate_model(y_avg_test, y_avg_pred, X_test)\n",
    "\n",
    "        future_predictions = generate_future_predictions(df, model_min, model_max, model_avg, days=5, start_date='2024-11-19', lag_features=lag_features)\n",
    "\n",
    "        output_text = f\"City: {city_name}\\n\"\n",
    "        output_text += f\"TMIN Evaluation: R²: {r2_min:.4f}, Adjusted R²: {adj_r2_min:.4f}, MSE: {mse_min:.4f}, RMSE: {rmse_min:.4f}\\n\"\n",
    "        output_text += f\"TMAX Evaluation: R²: {r2_max:.4f}, Adjusted R²: {adj_r2_max:.4f}, MSE: {mse_max:.4f}, RMSE: {rmse_max:.4f}\\n\"\n",
    "        output_text += f\"TAVG Evaluation: R²: {r2_avg:.4f}, Adjusted R²: {adj_r2_avg:.4f}, MSE: {mse_avg:.4f}, RMSE: {rmse_avg:.4f}\\n\\n\"\n",
    "        output_text += \"Future 5-Day Predictions:\\n\"\n",
    "        output_text += future_predictions.to_string(index=False)\n",
    "        output_text += \"\\n\\n\" + \"-\" * 50 + \"\\n\\n\"\n",
    "\n",
    "        with open(output_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(output_text)\n",
    "\n",
    "        print(f\"Processed {city_name}, results appended to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.Arima: The model models the time sequence modeling by capturing the self -regression (AR), differential (i), and mobile average (MA) components of data.\n",
    "Arima model model parameter selection: Use Auto_arima to automatically select parameters.\n",
    "Let's choose the right parameter first, through Auto_arima, we get Tmax, Order = (5, 1, 3), TMIN, Order = (5, 1, 1), tavg, order = (5, 1, 3), to ensure the guarantee The reliability of running time, we choose to reduce the training time window, and select the data of the first 600 days as a data set. At the same time, the model evaluation is reduced, and only MSE and MAE are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed KBNA, results appended to output/AllCities_ARIMA.txt\n",
      "Processed KBOI, results appended to output/AllCities_ARIMA.txt\n",
      "Processed KDCA, results appended to output/AllCities_ARIMA.txt\n",
      "Processed KDEN, results appended to output/AllCities_ARIMA.txt\n",
      "Processed KDTW, results appended to output/AllCities_ARIMA.txt\n",
      "Processed KIAH, results appended to output/AllCities_ARIMA.txt\n",
      "Processed KJFK, results appended to output/AllCities_ARIMA.txt\n",
      "Processed KMIA, results appended to output/AllCities_ARIMA.txt\n",
      "Processed KMSP, results appended to output/AllCities_ARIMA.txt\n",
      "Processed KOKC, results appended to output/AllCities_ARIMA.txt\n",
      "Processed KORD, results appended to output/AllCities_ARIMA.txt\n",
      "Processed KPDX, results appended to output/AllCities_ARIMA.txt\n",
      "Processed KPHX, results appended to output/AllCities_ARIMA.txt\n",
      "Processed KPWM, results appended to output/AllCities_ARIMA.txt\n",
      "Processed KSAN, results appended to output/AllCities_ARIMA.txt\n",
      "Processed KSEA, results appended to output/AllCities_ARIMA.txt\n",
      "Processed KSFO, results appended to output/AllCities_ARIMA.txt\n",
      "Processed KSLC, results appended to output/AllCities_ARIMA.txt\n",
      "Processed PANC, results appended to output/AllCities_ARIMA.txt\n",
      "Processed PHNL, results appended to output/AllCities_ARIMA.txt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def arima_forecast(series, order=(1, 1, 1), steps=5, start_date=None):\n",
    "    model = ARIMA(series, order=order)\n",
    "    model_fit = model.fit()\n",
    "    forecast = model_fit.forecast(steps=steps)\n",
    "    future_dates = [start_date + pd.Timedelta(days=i) for i in range(1, steps + 1)]\n",
    "    forecast_df = pd.DataFrame({'DATE': future_dates, 'FORECAST': forecast})\n",
    "    return forecast_df\n",
    "\n",
    "def evaluate_predictions(actual, predicted):\n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    mae = mean_absolute_error(actual, predicted)\n",
    "    return mse, mae\n",
    "\n",
    "directory = '.'  \n",
    "output_file = 'output/AllCities_ARIMA.txt' \n",
    "os.makedirs('output', exist_ok=True)  \n",
    "\n",
    "def combine_future_predictions(tmin_forecast_df, tmax_forecast_df, tavg_forecast_df):\n",
    "    \"\"\"Combine TMIN, TMAX, and TAVG forecasts into a single DataFrame.\"\"\"\n",
    "    combined_forecast = pd.DataFrame({\n",
    "        'DATE': tmin_forecast_df['DATE'],\n",
    "        'TMIN': tmin_forecast_df['FORECAST'],\n",
    "        'TMAX': tmax_forecast_df['FORECAST'],\n",
    "        'TAVG': tavg_forecast_df['FORECAST']\n",
    "    })\n",
    "    return combined_forecast\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        city_name = os.path.splitext(filename)[0]\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        df['DATE'] = pd.to_datetime(df[['YEAR', 'MONTH', 'DAY']], errors='coerce')\n",
    "        df = df[df['DATE'] <= '2024-11-19']  # Filter data before or on 2024-11-19\n",
    "\n",
    "        df['TMAX'] = df['TMAX'].replace(0, np.nan)\n",
    "        df['TMIN'] = df['TMIN'].replace(0, np.nan)\n",
    "        df['TAVG'] = (df['TMAX'] + df['TMIN']) / 2  \n",
    "        df['TMAX'] = df['TMAX'].interpolate(method='linear')\n",
    "        df['TMIN'] = df['TMIN'].interpolate(method='linear')\n",
    "        df['TAVG'] = df['TAVG'].interpolate(method='linear')\n",
    "\n",
    "        # Train-Test Split\n",
    "        train_tmax = df['TMAX'][-600:-5]\n",
    "        test_tmax = df['TMAX'][-5:]\n",
    "        train_tmin = df['TMIN'][-600:-5]\n",
    "        test_tmin = df['TMIN'][-5:]\n",
    "        train_tavg = df['TAVG'][-600:-5]\n",
    "        test_tavg = df['TAVG'][-5:]\n",
    "\n",
    "        start_date = df['DATE'].max()\n",
    "        tmax_arima_forecast_df = arima_forecast(train_tmax, order=(5, 1, 3), steps=5, start_date=start_date)\n",
    "        tmin_arima_forecast_df = arima_forecast(train_tmin, order=(5, 1, 1), steps=5, start_date=start_date)\n",
    "        tavg_arima_forecast_df = arima_forecast(train_tavg, order=(5, 1, 3), steps=5, start_date=start_date)\n",
    "\n",
    "        combined_forecast_df = combine_future_predictions(tmin_arima_forecast_df, tmax_arima_forecast_df, tavg_arima_forecast_df)\n",
    "\n",
    "        # Evaluate ARIMA predictions\n",
    "        tmax_arima_mse, tmax_arima_mae = evaluate_predictions(test_tmax, tmax_arima_forecast_df['FORECAST'])\n",
    "        tmin_arima_mse, tmin_arima_mae = evaluate_predictions(test_tmin, tmin_arima_forecast_df['FORECAST'])\n",
    "        tavg_arima_mse, tavg_arima_mae = evaluate_predictions(test_tavg, tavg_arima_forecast_df['FORECAST'])\n",
    "\n",
    "        output_text = f\"City: {city_name}\\n\"\n",
    "        output_text += f\"TMAX - MSE: {tmax_arima_mse:.2f}, MAE: {tmax_arima_mae:.2f}\\n\"\n",
    "        output_text += f\"TMIN - MSE: {tmin_arima_mse:.2f}, MAE: {tmin_arima_mae:.2f}\\n\"\n",
    "        output_text += f\"TAVG - MSE: {tavg_arima_mse:.2f}, MAE: {tavg_arima_mae:.2f}\\n\\n\"\n",
    "        output_text += \"Future 5-Day Predictions (Combined):\\n\"\n",
    "        output_text += combined_forecast_df.to_string(index=False)\n",
    "        output_text += \"\\n\\n\" + \"-\" * 50 + \"\\n\\n\"\n",
    "        \n",
    "        with open(output_file, 'a', encoding='utf-8') as f:\n",
    "            f.write(output_text)\n",
    "\n",
    "        print(f\"Processed {city_name}, results appended to {output_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
